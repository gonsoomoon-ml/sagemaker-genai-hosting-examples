{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ddeb2c-7877-46e3-9a4e-b2efb0d1b7a4",
   "metadata": {},
   "source": [
    "# How to optimize the Meta Llama-3 70B Amazon JumpStart model for inference using Amazon SageMaker model optimization jobs\n",
    "**Recommended kernel(s):** This notebook can be run with any Amazon SageMaker Studio kernel.\n",
    "\n",
    "In this notebook, you will learn how to apply state-of-the-art optimization techniques to an Amazon JumpStart model (JumpStart model ID: `meta-textgeneration-llama-3-70b`) using Amazon SageMaker ahead-of-time (AOT) model optimization capabilities. Each example includes the deployment of the optimized model to an Amazon SageMaker endpoint. In all cases, the inference image will be the SageMaker-managed [LMI (Large Model Inference)](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) Docker image. LMI images features a [DJL serving](https://github.com/deepjavalibrary/djl-serving) stack powered by the [Deep Java Library](https://djl.ai/). \n",
    "\n",
    "You will successively:\n",
    "1. Deploy a pre-optimized variant of the Amazon JumpStart model with speculative decoding enabled (using SageMaker provided draft model). For popular models, the JumpStart team indeed selects and applies the best optimization configurations for you.\n",
    "2. Customize the speculative decoding with open-source draft model.\n",
    "3. Quantize the model weights using the AWQ algorithm.\n",
    "4. Compile the model for a deployment of AWS Inferentia 2 accelerated hardware.\n",
    "\n",
    "**Notices:**\n",
    "* Make sure that the `ml.p4d.24xlarge` and `ml.inf2.48xlarge` instance types required for this tutorial are available in your AWS Region.\n",
    "* Make sure that the value of your \"ml.p4d.24xlarge for endpoint usage\" and \"ml.inf2.48xlarge for endpoint usage\" Amazon SageMaker service quotas allow you to deploy at least one Amazon SageMaker endpoint using these instance types.\n",
    "\n",
    "This notebook leverages the [Model Builder Class](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-modelbuilder-creation.html) within the [`sagemaker` Python SDK](https://sagemaker.readthedocs.io/en/stable/index.html) to abstract out container and model server management/tuning. Via the Model Builder Class you can easily interact with JumpStart Models, HuggingFace Hub Models, and also custom models via pointing towards an S3 path with your Model Data. For this sample we will focus on the JumpStart Optimization path.\n",
    "\n",
    "### License agreement\n",
    "* This model is under the Meta license, please refer to the original model card.\n",
    "* This notebook is a sample notebook and not intended for production use.\n",
    "\n",
    "### Execution environment setup\n",
    "This notebook requires the following third-party Python dependencies:\n",
    "* AWS [`boto3`](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html#)\n",
    "* AWS [`sagemaker`](https://sagemaker.readthedocs.io/en/stable/index.html) with a version greater than or equal to 2.225.0 \n",
    "\n",
    "Let's install or upgrade these dependencies using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a645403-0c3e-4062-9d16-ef0b1041fbe3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sagemaker>=2.225.0 boto3 huggingface_hub --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5631d3-1c16-4ad5-a42c-85a28cf9dd3e",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65310881-31a9-453e-9f7b-c79876824cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "from sagemaker.session import Session\n",
    "import logging\n",
    "import huggingface_hub\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d22b6d0b-5c6c-4014-b836-df0268e11feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = Session()\n",
    "\n",
    "artifacts_bucket_name = sagemaker_session.default_bucket()\n",
    "execution_role_arn = sagemaker_session.get_caller_identity_arn()\n",
    "\n",
    "js_model_id = \"meta-textgeneration-llama-3-70b\"\n",
    "gpu_instance_type = \"ml.p4d.24xlarge\"\n",
    "neuron_instance_type = \"ml.inf2.48xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eb26871-315d-425f-a5d2-b05627191700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = \"Hello, I'm a language model, and I'm here to help you with your English.\"\n",
    "\n",
    "sample_input = {\n",
    "    \"inputs\": \"Hello, I'm a language model,\",\n",
    "    \"parameters\": {\"max_new_tokens\": 128, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "\n",
    "sample_output = [{\"generated_text\": response}]\n",
    "\n",
    "schema_builder = SchemaBuilder(sample_input, sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09451b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SchemaBuilder(\n",
       "input_serializer=<sagemaker.serve.builder.schema_builder.JSONSerializerWrapper object at 0x7fe62015f040>\n",
       "output_serializer=<sagemaker.serve.builder.schema_builder.JSONSerializerWrapper object at 0x7fe533428c40>\n",
       "input_deserializer=<sagemaker.base_deserializers.JSONDeserializer object at 0x7fe5df08caf0>\n",
       "output_deserializer=<sagemaker.base_deserializers.JSONDeserializer object at 0x7fe5df08ca60>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81838b96-ca02-4892-b861-8cb0634fb167",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Deploy a pre-optimized deployment configuration with speculative decoding (SageMaker provided draft model)\n",
    "The `meta-textgeneration-llama-3-70b` JumpStart model is available with multiple pre-optimized deployment configuration. Optimized model artifacts for each configuration have already been created by the JumpStart team and a readily available for deployment. In this section, you will deploy one of theses pre-optimized configuration to an Amazon SageMaker endpoint. \n",
    "\n",
    "### What is speculative decoding?\n",
    "Speculative decoding is an inference optimization technique introduced by [Y. Leviathan et al. (ICML 2023)](https://arxiv.org/abs/2211.17192) used to accelerate the decoding process of large and therefore slow LLMs for latency-critical applications. The key idea is to use a smaller, less powerful but faster model called the ***draft model*** to generate candidate tokens that get validated by the larger, more powerful but slower model called the ***target model***. At each iteration, the draft model generates $K>1$ candidate tokens. Then, using a single forward pass of the larger target model, none, part, or all candidate tokens get accepted. The more aligned the selected draft model is with the target model, the better guesses it makes, the higher candidate token acceptance rate and therefore the higher the speed ups. The larger the size gap between the target and the draft model, the largest the potential speedups.\n",
    "\n",
    "Let's start by creating a `ModelBuilder` instance for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f490309f-da98-4489-8bab-d1ffde767370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_builder = ModelBuilder(\n",
    "    model=js_model_id,\n",
    "    schema_builder=schema_builder,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role_arn=execution_role_arn,\n",
    "    log_level=logging.ERROR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294805b8-f056-4a24-9d10-df32039d1193",
   "metadata": {},
   "source": [
    "For each optimization configuration, the JumpStart team has computed key performance metrics such as time-to-first-token (TTFT) latency and throughput for multiple hardwares and concurrent invocation intensities. Let's visualize these metrics using the `display_benchmark_metrics` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2eedd83-3104-4820-bb5c-6164b9d0477f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "INFO:sagemaker.jumpstart:Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "WARNING:sagemaker.jumpstart:Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "No instance type selected for inference hosting endpoint. Defaulting to ml.p4d.24xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.p4d.24xlarge.\n",
      "ModelBuilder: INFO:     JumpStart ID meta-textgeneration-llama-3-70b is packaged with Image URI: 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.28.0-lmi10.0.0-cu124\n",
      "ModelBuilder: INFO:     Building for DJL JumpStart Model ID...\n",
      "ModelBuilder: WARNING:     Unable to check docker volume utilization at the expected path /var/lib/docker. [Errno 2] No such file or directory: '/var/lib/docker'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "WARNING:sagemaker.jumpstart:Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Instance Type             | Config Name   |   Concurrent Users |   Latency, TTFT (P50 in sec) |   Throughput (P50 in tokens/sec/user) |\n",
      "|:--------------------------|:--------------|-------------------:|-----------------------------:|--------------------------------------:|\n",
      "| ml.g5.48xlarge            | lmi           |                  1 |                         2.02 |                                 18.80 |\n",
      "| ml.g5.48xlarge            | lmi           |                  2 |                         2.10 |                                 15.40 |\n",
      "| ml.g5.48xlarge            | lmi           |                  4 |                         2.15 |                                  9.40 |\n",
      "| ml.g5.48xlarge            | lmi           |                  8 |                         2.93 |                                  6.70 |\n",
      "| ml.p4d.24xlarge           | lmi           |                 64 |                         0.20 |                                  9.70 |\n",
      "| ml.p4d.24xlarge           | lmi           |                128 |                         4.01 |                                  8.10 |\n",
      "| ml.p4d.24xlarge (Default) | lmi           |                  1 |                         0.10 |                                 44.70 |\n",
      "| ml.p4d.24xlarge           | lmi           |                  2 |                         0.11 |                                 41.00 |\n",
      "| ml.p4d.24xlarge           | lmi           |                  4 |                         0.12 |                                 36.90 |\n",
      "| ml.p4d.24xlarge           | lmi           |                  8 |                         0.12 |                                 30.60 |\n",
      "| ml.p4d.24xlarge           | lmi           |                 16 |                         0.13 |                                 23.20 |\n",
      "| ml.p4d.24xlarge           | lmi           |                 32 |                         0.15 |                                 15.70 |\n",
      "| ml.p5.48xlarge            | lmi           |                 64 |                         0.10 |                                 18.30 |\n",
      "| ml.p5.48xlarge            | lmi           |                128 |                         0.12 |                                 11.50 |\n",
      "| ml.p5.48xlarge            | lmi           |                256 |                         3.33 |                                 10.00 |\n",
      "| ml.p5.48xlarge            | lmi           |                  1 |                         0.08 |                                 48.20 |\n",
      "| ml.p5.48xlarge            | lmi           |                  2 |                         0.08 |                                 48.20 |\n",
      "| ml.p5.48xlarge            | lmi           |                  4 |                         0.08 |                                 46.60 |\n",
      "| ml.p5.48xlarge            | lmi           |                  8 |                         0.09 |                                 43.50 |\n",
      "| ml.p5.48xlarge            | lmi           |                 16 |                         0.09 |                                 37.30 |\n",
      "| ml.p5.48xlarge            | lmi           |                 32 |                         0.09 |                                 29.20 |\n",
      "| ml.g5.48xlarge            | lmi-optimized |                  1 |                         2.25 |                                 49.70 |\n",
      "| ml.g5.48xlarge            | lmi-optimized |                  2 |                         2.28 |                                 21.10 |\n",
      "| ml.g5.48xlarge            | lmi-optimized |                  4 |                         2.37 |                                 14.10 |\n",
      "| ml.g5.48xlarge            | lmi-optimized |                  8 |                         2.48 |                                  7.90 |\n",
      "| ml.g6.48xlarge            | lmi-optimized |                  1 |                         0.95 |                                 31.30 |\n",
      "| ml.g6.48xlarge            | lmi-optimized |                  2 |                         1.03 |                                 17.90 |\n",
      "| ml.g6.48xlarge            | lmi-optimized |                  4 |                         1.08 |                                 15.20 |\n",
      "| ml.g6.48xlarge            | lmi-optimized |                  8 |                         1.18 |                                  9.80 |\n",
      "| ml.g6.48xlarge            | lmi-optimized |                 16 |                         1.92 |                                  5.20 |\n",
      "| ml.p4d.24xlarge           | lmi-optimized |                  1 |                         0.10 |                                137.40 |\n",
      "| ml.p4d.24xlarge           | lmi-optimized |                  2 |                         0.11 |                                109.20 |\n",
      "| ml.p4d.24xlarge           | lmi-optimized |                  4 |                         0.13 |                                 85.00 |\n",
      "| ml.p4d.24xlarge           | lmi-optimized |                  8 |                         0.13 |                                 60.30 |\n",
      "| ml.p4d.24xlarge           | lmi-optimized |                 16 |                         0.16 |                                 40.30 |\n",
      "| ml.p4d.24xlarge           | lmi-optimized |                 32 |                         0.20 |                                 23.60 |\n",
      "| ml.p4d.24xlarge           | lmi-optimized |                 64 |                         0.29 |                                 12.90 |\n",
      "| ml.p4d.24xlarge           | lmi-optimized |                128 |                         3.76 |                                 10.70 |\n",
      "| ml.p5.48xlarge            | lmi-optimized |                  1 |                         0.08 |                                136.40 |\n",
      "| ml.p5.48xlarge            | lmi-optimized |                  2 |                         0.08 |                                152.00 |\n",
      "| ml.p5.48xlarge            | lmi-optimized |                  4 |                         0.08 |                                134.20 |\n",
      "| ml.p5.48xlarge            | lmi-optimized |                  8 |                         0.09 |                                119.00 |\n",
      "| ml.p5.48xlarge            | lmi-optimized |                 16 |                         0.09 |                                 89.40 |\n",
      "| ml.p5.48xlarge            | lmi-optimized |                 32 |                         0.10 |                                 61.80 |\n",
      "| ml.p5.48xlarge            | lmi-optimized |                 64 |                         0.11 |                                 37.70 |\n",
      "| ml.p5.48xlarge            | lmi-optimized |                128 |                         0.15 |                                 18.30 |\n",
      "| ml.p5.48xlarge            | lmi-optimized |                256 |                         1.88 |                                 17.70 |\n",
      "| ml.p5.48xlarge            | lmi-optimized |                512 |                         3.76 |                                 51.10 |\n"
     ]
    }
   ],
   "source": [
    "model_builder.display_benchmark_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f84c42-d578-4136-9e86-34058e134707",
   "metadata": {},
   "source": [
    "Now, let's pick and deploy the `lmi-optimized` pre-optimized configuration to a `ml.p4d.24xlarge` instance. The `lmi-optimized` configuration enables speculative decoding. In this configuration, a SageMaker provided draft model is used. Therefore, you don't have to supply a draft model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e9bb346-4108-4721-86bc-c754d59c4bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_builder.set_deployment_config(config_name=\"lmi-optimized\", instance_type=gpu_instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0145648e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ml.p4d.24xlarge'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_instance_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35232c11-eb78-4521-ae60-243bdc7e1666",
   "metadata": {},
   "source": [
    "Currently set deployment configuration can be visualized using the `get_deployment_config` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb8e44a7-cce1-4417-b701-6895638fa42e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "WARNING:sagemaker.jumpstart:Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'DeploymentConfigName': 'lmi-optimized',\n",
       " 'DeploymentArgs': {'ImageUri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.28.0-lmi10.0.0-cu124',\n",
       "  'ModelData': {'S3DataSource': {'S3Uri': 's3://jumpstart-private-cache-prod-us-east-1/meta-textgeneration/meta-textgeneration-llama-3-70b/artifacts/inference-prepack/v1.1.0/',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'CompressionType': 'None'}},\n",
       "  'ModelPackageArn': None,\n",
       "  'Environment': {'SAGEMAKER_PROGRAM': 'inference.py',\n",
       "   'ENDPOINT_SERVER_TIMEOUT': '3600',\n",
       "   'MODEL_CACHE_ROOT': '/opt/ml/model',\n",
       "   'SAGEMAKER_ENV': '1',\n",
       "   'HF_MODEL_ID': '/opt/ml/model',\n",
       "   'OPTION_SPECULATIVE_DRAFT_MODEL': '/opt/ml/additional-model-data-sources/draft_model',\n",
       "   'SAGEMAKER_MODEL_SERVER_WORKERS': '1',\n",
       "   'OPTION_GPU_MEMORY_UTILIZATION': '0.65'},\n",
       "  'InstanceType': 'ml.p4d.24xlarge',\n",
       "  'ComputeResourceRequirements': {'MinMemoryRequiredInMb': 589824,\n",
       "   'NumberOfAcceleratorDevicesRequired': 8},\n",
       "  'ModelDataDownloadTimeout': 1200,\n",
       "  'ContainerStartupHealthCheckTimeout': 1200,\n",
       "  'AdditionalDataSources': {'speculative_decoding': [{'channel_name': 'draft_model',\n",
       "     'artifact_version': 'v3',\n",
       "     's3_data_source': {'compression_type': 'None',\n",
       "      's3_data_type': 'S3Prefix',\n",
       "      's3_uri': 'sagemaker-speculative-decoding-llama3-small-v3/'}}]}},\n",
       " 'AccelerationConfigs': [{'type': 'Compilation', 'enabled': False},\n",
       "  {'type': 'Speculative-Decoding', 'enabled': True},\n",
       "  {'type': 'Quantization', 'enabled': False}],\n",
       " 'BenchmarkMetrics': {'ml.p4d.24xlarge': [{'name': 'Latency',\n",
       "    'value': '0.10',\n",
       "    'unit': 'sec',\n",
       "    'concurrency': '1'},\n",
       "   {'name': 'Throughput',\n",
       "    'value': '137.4',\n",
       "    'unit': 'tokens/sec',\n",
       "    'concurrency': '1'},\n",
       "   {'name': 'Latency', 'value': '0.11', 'unit': 'sec', 'concurrency': '2'},\n",
       "   {'name': 'Throughput',\n",
       "    'value': '109.2',\n",
       "    'unit': 'tokens/sec',\n",
       "    'concurrency': '2'},\n",
       "   {'name': 'Latency', 'value': '0.13', 'unit': 'sec', 'concurrency': '4'},\n",
       "   {'name': 'Throughput',\n",
       "    'value': '85.0',\n",
       "    'unit': 'tokens/sec',\n",
       "    'concurrency': '4'},\n",
       "   {'name': 'Latency', 'value': '0.13', 'unit': 'sec', 'concurrency': '8'},\n",
       "   {'name': 'Throughput',\n",
       "    'value': '60.3',\n",
       "    'unit': 'tokens/sec',\n",
       "    'concurrency': '8'},\n",
       "   {'name': 'Latency', 'value': '0.16', 'unit': 'sec', 'concurrency': '16'},\n",
       "   {'name': 'Throughput',\n",
       "    'value': '40.3',\n",
       "    'unit': 'tokens/sec',\n",
       "    'concurrency': '16'},\n",
       "   {'name': 'Latency', 'value': '0.20', 'unit': 'sec', 'concurrency': '32'},\n",
       "   {'name': 'Throughput',\n",
       "    'value': '23.6',\n",
       "    'unit': 'tokens/sec',\n",
       "    'concurrency': '32'},\n",
       "   {'name': 'Latency', 'value': '0.29', 'unit': 'sec', 'concurrency': '64'},\n",
       "   {'name': 'Throughput',\n",
       "    'value': '12.9',\n",
       "    'unit': 'tokens/sec',\n",
       "    'concurrency': '64'},\n",
       "   {'name': 'Latency', 'value': '3.76', 'unit': 'sec', 'concurrency': '128'},\n",
       "   {'name': 'Throughput',\n",
       "    'value': '10.7',\n",
       "    'unit': 'tokens/sec',\n",
       "    'concurrency': '128'}]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_builder.get_deployment_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad37a7b-e2cc-45af-bf27-ea6c1aa258ba",
   "metadata": {},
   "source": [
    "Now, let's build the `Model` instance and use it to deploy the selected optimized configuration. This operation may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "972aea37-4a16-4eee-b3ad-33b1077cbcbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: INFO:     Either inference spec or model is provided. ModelBuilder is not handling MLflow model input\n",
      "Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "INFO:sagemaker.jumpstart:Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "WARNING:sagemaker.jumpstart:Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "ModelBuilder: INFO:     JumpStart Model ID detected.\n",
      "ModelBuilder: INFO:     JumpStart Model ID detected.\n"
     ]
    }
   ],
   "source": [
    "optimized_model = model_builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25524252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.jumpstart.model.JumpStartModel at 0x7fe62015f0a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f27a2527-a189-4b26-b579-bae031f69248",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "INFO:sagemaker.jumpstart:Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "WARNING:sagemaker.jumpstart:Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-3-70b-2024-07-21-01-53-51-512\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-3-70b-2024-07-21-01-53-51-920\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-3-70b-2024-07-21-01-53-51-920\n",
      "INFO:sagemaker:CUDA compat package requires Nvidia driver ⩽550.90.07\n",
      "INFO:sagemaker:Current installed Nvidia driver version is 535.183.01\n",
      "INFO:sagemaker:Setup CUDA compatibility libs path to LD_LIBRARY_PATH\n",
      "INFO:sagemaker:/usr/local/cuda/compat:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "INFO:sagemaker:[INFO ] ModelServer - Starting model server ...\n",
      "INFO:sagemaker:[INFO ] ModelServer - Starting djl-serving: 0.28.0 ...\n",
      "INFO:sagemaker:[INFO ] ModelServer - \n",
      "INFO:sagemaker:Model server home: /opt/djl\n",
      "INFO:sagemaker:Current directory: /opt/djl\n",
      "INFO:sagemaker:Temp directory: /tmp\n",
      "INFO:sagemaker:Command line: -Dlog4j.configurationFile=/usr/local/djl-serving-0.28.0/conf/log4j2-plain.xml -Xmx1g -Xms1g -XX:+ExitOnOutOfMemoryError -Dai.djl.util.cuda.fork=true -XX:-UseContainerSupport\n",
      "INFO:sagemaker:Number of CPUs: 96\n",
      "INFO:sagemaker:CUDA version: 124 / 80\n",
      "INFO:sagemaker:Number of GPUs: 8\n",
      "INFO:sagemaker:Max heap size: 1024\n",
      "INFO:sagemaker:Config file: /opt/djl/conf/config.properties\n",
      "INFO:sagemaker:Inference address: http://0.0.0.0:8080\n",
      "INFO:sagemaker:Management address: http://0.0.0.0:8080\n",
      "INFO:sagemaker:Default job_queue_size: 1000\n",
      "INFO:sagemaker:Default batch_size: 1\n",
      "INFO:sagemaker:Default max_batch_delay: 100\n",
      "INFO:sagemaker:Default max_idle_time: 60\n",
      "INFO:sagemaker:Model Store: /opt/ml/model\n",
      "INFO:sagemaker:Initial Models: ALL\n",
      "INFO:sagemaker:Netty threads: 0\n",
      "INFO:sagemaker:Maximum Request Size: 67108864\n",
      "INFO:sagemaker:Environment variables:\n",
      "    HF_HUB_ENABLE_HF_TRANSFER: 1\n",
      "    HF_HOME: /tmp/.cache/huggingface\n",
      "    OPTION_SPECULATIVE_DRAFT_MODEL: /opt/ml/additional-model-data-sources/draft_model\n",
      "    OMP_NUM_THREADS: 1\n",
      "    OPTION_GPU_MEMORY_UTILIZATION: 0.65\n",
      "    SAGEMAKER_SAFE_PORT_RANGE: 16000-16999\n",
      "    HF_MODEL_ID: /opt/ml/model\n",
      "    SAGEMAKER_TRUSTED_CHANNELS: /opt/ml/additional-model-data-sources/draft_model\n",
      "    SERVING_FEATURES: vllm,lmi-dist\n",
      "    DJL_CACHE_DIR: /tmp/.djl.ai\n",
      "    SAGEMAKER_MODEL_SERVER_WORKERS: 1\n",
      "    SAGEMAKER_ADDITIONAL_MODEL_DATA: /opt/ml/additional-model-data-sources/draft_model\n",
      "    SAGEMAKER_ENV: 1\n",
      "    SAGEMAKER_SECURE_MODE: true\n",
      "    SAGEMAKER_UNTRUSTED_CHANNELS: /opt/ml/model\n",
      "    SAGEMAKER_SECURITY_CONTROLS: DISALLOW_REQUIREMENTS_TXT,DISALLOW_CUSTOM_INFERENCE_SCRIPTS,DISALLOW_PICKLE_FILES,DISALLOW_TRUST_REMOTE_CODE\n",
      "    SAGEMAKER_PROGRAM: inference.py\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - scanning for plugins...\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - scanning in plug-in folder :/opt/djl/plugins\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - scanning in plug-in folder :/usr/local/djl-serving-0.28.0/plugins\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: cache-engines/jar:file:/usr/local/djl-serving-0.28.0/plugins/cache-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: static-file-plugin/jar:file:/usr/local/djl-serving-0.28.0/plugins/static-file-plugin-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: secure-mode/jar:file:/usr/local/djl-serving-0.28.0/plugins/secure-mode-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: console/jar:file:/usr/local/djl-serving-0.28.0/plugins/management-console-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: kserve/jar:file:/usr/local/djl-serving-0.28.0/plugins/kserve-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {console/jar:file:/usr/local/djl-serving-0.28.0/plugins/management-console-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin console changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {static-file-plugin/jar:file:/usr/local/djl-serving-0.28.0/plugins/static-file-plugin-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin static-file-plugin changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {cache-engines/jar:file:/usr/local/djl-serving-0.28.0/plugins/cache-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin cache-engines changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {secure-mode/jar:file:/usr/local/djl-serving-0.28.0/plugins/secure-mode-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin secure-mode changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {kserve/jar:file:/usr/local/djl-serving-0.28.0/plugins/kserve-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin kserve changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin console changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin static-file-plugin changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin cache-engines changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin secure-mode changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin kserve changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - 5 plug-ins found and loaded.\n",
      "INFO:sagemaker:[INFO ] ModelServer - Found model model=file:/opt/ml/model/\n",
      "INFO:sagemaker:[INFO ] ModelServer - Found model model=file:/opt/ml/model/\n",
      "INFO:sagemaker:[INFO ] ModelServer - Initializing model: model=file:/opt/ml/model/\n",
      "INFO:sagemaker:[INFO ] LmiUtils - Detected mpi_mode: true, rolling_batch: lmi-dist, tensor_parallel_degree 8, for modelType: llama\n",
      "INFO:sagemaker:[INFO ] ModelInfo - M-0001: Apply per model settings:\n",
      "    job_queue_size: 1000\n",
      "    max_dynamic_batch_size: 1\n",
      "    max_batch_delay: 100\n",
      "    max_idle_time: 60\n",
      "    load_on_devices: *\n",
      "    engine: Python\n",
      "    mpi_mode: true\n",
      "    option.entryPoint: null\n",
      "    option.tensor_parallel_degree: 8\n",
      "    option.speculative_draft_model: /opt/ml/additional-model-data-sources/draft_model\n",
      "    option.max_rolling_batch_size: 256\n",
      "    option.mpi_mode: true\n",
      "    option.gpu_memory_utilization: 0.65\n",
      "    option.rolling_batch: lmi-dist\n",
      "INFO:sagemaker:[INFO ] SecureModeUtils - Secure Mode enabled with the following security controls: DISALLOW_REQUIREMENTS_TXT,DISALLOW_CUSTOM_INFERENCE_SCRIPTS,DISALLOW_PICKLE_FILES,DISALLOW_TRUST_REMOTE_CODE\n",
      "INFO:sagemaker:[INFO ] Platform - Found matching platform from: jar:file:/usr/local/djl-serving-0.28.0/lib/python-0.28.0.jar!/native/lib/python.properties\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python_engine.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/arg_parser.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/aws/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/aws/cloud_watch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/chat_completions/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/chat_completions/chat_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/chat_completions/chat_utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/encode_decode.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/huggingface.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/inputs.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/neuron_utils/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/neuron_utils/model_loader.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/neuron_utils/utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/np_util.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/output_formatter.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/outputs.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/pair_list.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/README.md to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/hf_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/lmi_dist_rb_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/scheduler_rb_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/sd_inf2_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/tnx_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/trt_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/vllm_rb_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/request.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/request_io.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/lmi_dist_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/neuron_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/rolling_batch_vllm_utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/scheduler_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/trtllm_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/vllm_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/sagemaker.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/lm_block.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/search_config.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/seq_batch_scheduler.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/seq_batcher.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/seq_batcher_impl.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/step_generation.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/service_loader.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/session_manager.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/sm_log_filter.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/stable_diffusion_inf2.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/streaming_utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/telemetry.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/tensorrt_llm.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/tensorrt_llm_python.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/test_model.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/optimum_modeling.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/optimum_neuron_scheduler.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/slot.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/speculation.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/token_selector.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/ts_service_loader.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] ModelManager - Loading model on Python:[0]\n",
      "INFO:sagemaker:[INFO ] WorkerPool - loading model model (M-0001, PENDING) on gpu(0) ...\n",
      "INFO:sagemaker:[INFO ] ModelInfo - M-0001: Available CPU memory: 1128811 MB, required: 0 MB, reserved: 500 MB\n",
      "INFO:sagemaker:[INFO ] ModelInfo - M-0001: Available GPU memory: 39916 MB, required: 0 MB, reserved: 500 MB\n",
      "INFO:sagemaker:[INFO ] ModelInfo - Loading model model M-0001 on gpu(0)\n",
      "INFO:sagemaker:[INFO ] PyModel - Loading model in MPI mode with TP: 8.\n",
      "INFO:sagemaker:[INFO ] PyModel - Start 1 mpiWorkers ...\n",
      "INFO:sagemaker:[INFO ] PyProcess - Start process: 19000 - retry: 0\n",
      "INFO:sagemaker:[INFO ] Connection - Set CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,4]<stdout>:298 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,6]<stdout>:300 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:294 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,1]<stdout>:295 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,2]<stdout>:296 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,5]<stdout>:299 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,7]<stdout>:301 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,3]<stdout>:297 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,5]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,3]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,6]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,1]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,7]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,4]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,2]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,2]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,7]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,5]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,3]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,1]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,6]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,4]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,1]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,2]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,3]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,4]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,5]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,6]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,7]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:00:09 arg_utils.py:22] Found draft_model parameter: /opt/ml/additional-model-data-sources/draft_model, will apply speculative patch\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:00:09 utils.py:660] Found nccl from library /opt/djl/vllm/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:WARNING 07-21 02:00:09 arg_utils.py:26] The draft model's max context length (4096) is not equals target model (8192), the max context length is now set to 4096.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:00:09 vllm_engine.py:68] Using speculative decoding for LLM engine: draft_model='/opt/ml/additional-model-data-sources/draft_model', speculate_length=5, draft_model_tp_size=1, quantization=None\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:00:09 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,5]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,6]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,3]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,2]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,7]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,4]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,1]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,0]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,0]<stderr>:[rank0]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,3]<stderr>:[rank3]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,1]<stderr>:[rank1]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,4]<stderr>:[rank4]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,2]<stderr>:[rank2]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,5]<stderr>:[rank5]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,7]<stderr>:[rank7]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,6]<stderr>:[rank6]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,0]<stderr>:[rank0]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,0]<stderr>:[rank0]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,4]<stderr>:[rank4]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,3]<stderr>:[rank3]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,1]<stderr>:[rank1]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,4]<stderr>:[rank4]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:00:11 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,2]<stderr>:[rank2]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,3]<stderr>:[rank3]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,1]<stderr>:[rank1]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,7]<stderr>:[rank7]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,5]<stderr>:[rank5]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,6]<stderr>:[rank6]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,2]<stderr>:[rank2]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,5]<stderr>:[rank5]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,7]<stderr>:[rank7]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,6]<stderr>:[rank6]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:00:23 utils.py:118] generating GPU P2P access cache for in /opt/djl/vllm/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:00:24 utils.py:132] reading GPU P2P access cache from /opt/djl/vllm/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,5]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,5]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:00:42 model_runner.py:175] Loading model weights took 16.4312 GB\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:00:42 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:00:43 model_runner.py:35] Loading draft model weights took 0.4147 GB\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,1]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,6]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,0]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,2]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,4]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,7]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,3]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,1]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,6]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,0]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,2]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,4]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,7]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,3]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:00:47 vllm_engine.py:62] # GPU blocks: 6904, # CPU blocks: 6393\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:00:50 model_runner.py:39] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:00:50 model_runner.py:39] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:00:54 custom_all_reduce.py:246] Registering 0 cuda graph addresses\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:01:00 custom_all_reduce.py:246] Registering 3059 cuda graph addresses\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:01:01 model_runner.py:51] Graph capturing finished in 11 secs.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:01:01 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:01:01 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:01:12 custom_all_reduce.py:246] Registering 5635 cuda graph addresses\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:01:12 model_runner.py:1017] Graph capturing finished in 11 secs.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-288-model-stdout: [1,0]<stdout>:INFO 07-21 02:01:12 worker.py:38] CUDA graphs took took 0.3691 (speculation) + 0.4023 (fallback) = 0.7715 GB\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,0]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,3]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,5]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,7]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,2]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,6]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,1]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-288-model-stderr: [1,4]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[INFO ] PyProcess - Model [model] initialized.\n",
      "INFO:sagemaker:[INFO ] PyModel - model model loaded in 71070 ms.\n",
      "INFO:sagemaker:[INFO ] WorkerPool - scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "INFO:sagemaker:[INFO ] WorkerThread - Starting worker thread WT-0001 for model model (M-0001, READY) on device gpu(0)\n",
      "INFO:sagemaker:[INFO ] ModelServer - Initialize BOTH server with: EpollServerSocketChannel.\n",
      "INFO:sagemaker:[INFO ] ModelServer - BOTH API bind to: http://0.0.0.0:8080\n",
      "INFO:sagemaker:Created endpoint with name meta-textgeneration-llama-3-70b-2024-07-21-01-53-51-920\n",
      "ModelBuilder: DEBUG:     ModelBuilder metrics emitted.\n"
     ]
    }
   ],
   "source": [
    "predictor = optimized_model.deploy(accept_eula=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e7fb08-3ff0-40c5-8a4e-9a026d7fba62",
   "metadata": {},
   "source": [
    "Once the deployment has finished successfully, you can send queries to the model by simply using the predictor's `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaa8249e-9e8a-48d1-9dd2-2a31d664d954",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': \" and I'm here to help you with your questions about the world of technology. Today, I want to talk about the importance of having a strong online presence for your business. In today's digital age, having a website is no longer a luxury, but a necessity. It's the first place potential customers will go to learn more about your business, and it's essential to make a good first impression.\\nBut having a website is just the first step. You also need to make sure that your website is optimized for search engines, so that it can be easily found by potential customers. This is where search engine optimization (SEO) comes in\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7984819c-e3ec-47d9-92a8-d91fa4998b55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: meta-textgeneration-llama-3-70b-2024-07-21-01-53-51-512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: meta-textgeneration-llama-3-70b-2024-07-21-01-53-51-920\n",
      "INFO:sagemaker:Deleting endpoint with name: meta-textgeneration-llama-3-70b-2024-07-21-01-53-51-920\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf4750b-de17-4764-a93e-bba011b84613",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Customize the speculative decoding with open-source draft model, then deploy the optimized model\n",
    "In this section and instead of relying on a pre-optimized model, you will use an Amazon SageMaker optimization toolkit to enable speculative decoding on the `meta-textgeneration-llama-3-70b` JumpStart model. In this example, the draft model is from HuggingFace model hub. We use the HF-Hub model package to download these artifacts to S3 directly, optionally you can also provide your HF Model ID. In this case for the draft model we use Meta-Llama-3-8B, for this model ensure you have access to the artifacts via HF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e738873f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99407c33ed514a9ca426012c37c3b8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Hugging Face에 로그인\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f25f97b7-cc09-430e-8174-9610a3364ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1212: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65364b3178794c968cca784c53348191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/sagemaker-user/sagemaker-genai-hosting-examples-1/Llama3/llama3-70b/model_repo'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_draft_model_id=\"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "hf_local_download_dir = Path.cwd() / \"model_repo\"\n",
    "hf_local_download_dir.mkdir(exist_ok=True)\n",
    "\n",
    "huggingface_hub.snapshot_download(\n",
    "    repo_id=custom_draft_model_id,\n",
    "    revision=\"main\",\n",
    "    local_dir=hf_local_download_dir,\n",
    "    local_dir_use_symlinks=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18e89c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sagemaker-user/sagemaker-genai-hosting-examples-1/Llama3/llama3-70b/model_repo'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_local_download_dir.as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b20facae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 15693168\n",
      "drwxr-xr-x 4 sagemaker-user users       4096 Jul 20 14:31 .\n",
      "drwxr-xr-x 3 sagemaker-user users        105 Jul 20 14:30 ..\n",
      "drwxr-xr-x 3 sagemaker-user users         25 Jul 20 14:28 .cache\n",
      "-rw-r--r-- 1 sagemaker-user users       1519 Jul 20 14:28 .gitattributes\n",
      "-rw-r--r-- 1 sagemaker-user users       7801 Jul 20 14:28 LICENSE\n",
      "-rw-r--r-- 1 sagemaker-user users      36547 Jul 20 14:28 README.md\n",
      "-rw-r--r-- 1 sagemaker-user users       4696 Jul 20 14:28 USE_POLICY.md\n",
      "-rw-r--r-- 1 sagemaker-user users        654 Jul 20 14:28 config.json\n",
      "-rw-r--r-- 1 sagemaker-user users        177 Jul 20 14:28 generation_config.json\n",
      "-rw-r--r-- 1 sagemaker-user users 4976698672 Jul 20 14:30 model-00001-of-00004.safetensors\n",
      "-rw-r--r-- 1 sagemaker-user users 4999802720 Jul 20 14:30 model-00002-of-00004.safetensors\n",
      "-rw-r--r-- 1 sagemaker-user users 4915916176 Jul 20 14:30 model-00003-of-00004.safetensors\n",
      "-rw-r--r-- 1 sagemaker-user users 1168138808 Jul 20 14:29 model-00004-of-00004.safetensors\n",
      "-rw-r--r-- 1 sagemaker-user users      23950 Jul 20 14:28 model.safetensors.index.json\n",
      "drwxr-xr-x 2 sagemaker-user users         75 Jul 20 14:31 original\n",
      "-rw-r--r-- 1 sagemaker-user users         73 Jul 20 14:28 special_tokens_map.json\n",
      "-rw-r--r-- 1 sagemaker-user users    9085698 Jul 20 14:28 tokenizer.json\n",
      "-rw-r--r-- 1 sagemaker-user users      50566 Jul 20 14:28 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "! ls -al '/home/sagemaker-user/sagemaker-genai-hosting-examples-1/Llama3/llama3-70b/model_repo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84189520-f0b4-432c-9ce3-4c610e8016cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_draft_model_uri = sagemaker_session.upload_data(\n",
    "    path=hf_local_download_dir.as_posix(),\n",
    "    bucket=artifacts_bucket_name,\n",
    "    key_prefix=\"spec-dec-custom-draft-model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32e0c691-ea21-4c3d-9654-6329c115ede3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-057716757052/spec-dec-custom-draft-model/'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draft_uri = custom_draft_model_uri + \"/\" #need to point towards the uncompressed model artifacts\n",
    "draft_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90fc86b1-8eaf-42a2-a911-ac962ac5a33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE .cache/\n",
      "                           PRE original/\n",
      "2024-07-21 02:01:59       1519 .gitattributes\n",
      "2024-07-21 02:01:59       7801 LICENSE\n",
      "2024-07-21 02:01:59      36547 README.md\n",
      "2024-07-21 02:01:59       4696 USE_POLICY.md\n",
      "2024-07-21 02:01:59        654 config.json\n",
      "2024-07-21 02:01:59        177 generation_config.json\n",
      "2024-07-21 02:03:35 4976698672 model-00001-of-00004.safetensors\n",
      "2024-07-21 02:02:53 4999802720 model-00002-of-00004.safetensors\n",
      "2024-07-21 02:02:11 4915916176 model-00003-of-00004.safetensors\n",
      "2024-07-21 02:02:00 1168138808 model-00004-of-00004.safetensors\n",
      "2024-07-21 02:01:59      23950 model.safetensors.index.json\n",
      "2024-07-21 02:01:59         73 special_tokens_map.json\n",
      "2024-07-21 02:01:59    9085698 tokenizer.json\n",
      "2024-07-21 02:01:59      50566 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {draft_uri} #verify model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccb6b5db-5c86-4b57-a977-503232f9b6d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_builder = ModelBuilder(\n",
    "    model=js_model_id,\n",
    "    schema_builder=schema_builder,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role_arn=execution_role_arn,\n",
    "    log_level=logging.ERROR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb872c31-e77e-4c15-8c79-ce3812b39f9e",
   "metadata": {},
   "source": [
    "The optimization operation may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc7e51ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ml.p4d.24xlarge'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_instance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b503949-3ff5-4516-9f46-125ed1de652c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "INFO:sagemaker.jumpstart:Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "WARNING:sagemaker.jumpstart:Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "ModelBuilder: INFO:     JumpStart Model ID detected.\n",
      "ModelBuilder: INFO:     Either inference spec or model is provided. ModelBuilder is not handling MLflow model input\n",
      "ModelBuilder: INFO:     JumpStart Model ID detected.\n",
      "ModelBuilder: INFO:     JumpStart Model ID detected.\n",
      "Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "INFO:sagemaker.jumpstart:Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "WARNING:sagemaker.jumpstart:Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "No instance type selected for inference hosting endpoint. Defaulting to ml.p4d.24xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.p4d.24xlarge.\n",
      "ModelBuilder: INFO:     JumpStart ID meta-textgeneration-llama-3-70b is packaged with Image URI: 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.28.0-lmi10.0.0-cu124\n",
      "ModelBuilder: INFO:     Building for DJL JumpStart Model ID...\n",
      "ModelBuilder: WARNING:     Unable to check docker volume utilization at the expected path /var/lib/docker. [Errno 2] No such file or directory: '/var/lib/docker'\n",
      "Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "WARNING:sagemaker.jumpstart:Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "WARNING:sagemaker.jumpstart:Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "ModelBuilder: DEBUG:     ModelBuilder metrics emitted.\n"
     ]
    }
   ],
   "source": [
    "optimized_model = model_builder.optimize(\n",
    "    instance_type=gpu_instance_type,\n",
    "    accept_eula=True,\n",
    "    speculative_decoding_config={\n",
    "        \"ModelSource\": draft_uri\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b559d-49e7-4b96-8760-db3439fb325c",
   "metadata": {},
   "source": [
    "Now let's deploy the quantized model to an Amazon SageMaker endpoint. This operation may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e9e9a77-e7bc-4690-8daf-d77685116dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-3-70b-2024-07-21-02-06-38-904\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-3-70b-2024-07-21-02-06-39-418\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-3-70b-2024-07-21-02-06-39-418\n",
      "INFO:sagemaker:CUDA compat package requires Nvidia driver ⩽550.90.07\n",
      "INFO:sagemaker:Current installed Nvidia driver version is 535.183.01\n",
      "INFO:sagemaker:Setup CUDA compatibility libs path to LD_LIBRARY_PATH\n",
      "INFO:sagemaker:/usr/local/cuda/compat:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "INFO:sagemaker:[INFO ] ModelServer - Starting model server ...\n",
      "INFO:sagemaker:[INFO ] ModelServer - Starting djl-serving: 0.28.0 ...\n",
      "INFO:sagemaker:[INFO ] ModelServer - \n",
      "INFO:sagemaker:Model server home: /opt/djl\n",
      "INFO:sagemaker:Current directory: /opt/djl\n",
      "INFO:sagemaker:Temp directory: /tmp\n",
      "INFO:sagemaker:Command line: -Dlog4j.configurationFile=/usr/local/djl-serving-0.28.0/conf/log4j2-plain.xml -Xmx1g -Xms1g -XX:+ExitOnOutOfMemoryError -Dai.djl.util.cuda.fork=true -XX:-UseContainerSupport\n",
      "INFO:sagemaker:Number of CPUs: 96\n",
      "INFO:sagemaker:CUDA version: 124 / 80\n",
      "INFO:sagemaker:Number of GPUs: 8\n",
      "INFO:sagemaker:Max heap size: 1024\n",
      "INFO:sagemaker:Config file: /opt/djl/conf/config.properties\n",
      "INFO:sagemaker:Inference address: http://0.0.0.0:8080\n",
      "INFO:sagemaker:Management address: http://0.0.0.0:8080\n",
      "INFO:sagemaker:Default job_queue_size: 1000\n",
      "INFO:sagemaker:Default batch_size: 1\n",
      "INFO:sagemaker:Default max_batch_delay: 100\n",
      "INFO:sagemaker:Default max_idle_time: 60\n",
      "INFO:sagemaker:Model Store: /opt/ml/model\n",
      "INFO:sagemaker:Initial Models: ALL\n",
      "INFO:sagemaker:Netty threads: 0\n",
      "INFO:sagemaker:Maximum Request Size: 67108864\n",
      "INFO:sagemaker:Environment variables:\n",
      "    HF_HUB_ENABLE_HF_TRANSFER: 1\n",
      "    HF_HOME: /tmp/.cache/huggingface\n",
      "    OPTION_SPECULATIVE_DRAFT_MODEL: /opt/ml/additional-model-data-sources/draft_model\n",
      "    OMP_NUM_THREADS: 1\n",
      "    SAGEMAKER_SAFE_PORT_RANGE: 14000-14999\n",
      "    HF_MODEL_ID: /opt/ml/model\n",
      "    SERVING_FEATURES: vllm,lmi-dist\n",
      "    DJL_CACHE_DIR: /tmp/.djl.ai\n",
      "    SAGEMAKER_MODEL_SERVER_WORKERS: 1\n",
      "    SAGEMAKER_ENV: 1\n",
      "    SAGEMAKER_PROGRAM: inference.py\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - scanning for plugins...\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - scanning in plug-in folder :/opt/djl/plugins\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - scanning in plug-in folder :/usr/local/djl-serving-0.28.0/plugins\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: kserve/jar:file:/usr/local/djl-serving-0.28.0/plugins/kserve-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: secure-mode/jar:file:/usr/local/djl-serving-0.28.0/plugins/secure-mode-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: cache-engines/jar:file:/usr/local/djl-serving-0.28.0/plugins/cache-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: static-file-plugin/jar:file:/usr/local/djl-serving-0.28.0/plugins/static-file-plugin-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: console/jar:file:/usr/local/djl-serving-0.28.0/plugins/management-console-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {console/jar:file:/usr/local/djl-serving-0.28.0/plugins/management-console-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin console changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {static-file-plugin/jar:file:/usr/local/djl-serving-0.28.0/plugins/static-file-plugin-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin static-file-plugin changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {cache-engines/jar:file:/usr/local/djl-serving-0.28.0/plugins/cache-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin cache-engines changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {secure-mode/jar:file:/usr/local/djl-serving-0.28.0/plugins/secure-mode-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin secure-mode changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {kserve/jar:file:/usr/local/djl-serving-0.28.0/plugins/kserve-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin kserve changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin console changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin static-file-plugin changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin cache-engines changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin secure-mode changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin kserve changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - 5 plug-ins found and loaded.\n",
      "INFO:sagemaker:[INFO ] ModelServer - Found model model=file:/opt/ml/model/\n",
      "INFO:sagemaker:[INFO ] ModelServer - Found model model=file:/opt/ml/model/\n",
      "INFO:sagemaker:[INFO ] ModelServer - Initializing model: model=file:/opt/ml/model/\n",
      "INFO:sagemaker:[INFO ] LmiUtils - Detected mpi_mode: true, rolling_batch: lmi-dist, tensor_parallel_degree 8, for modelType: llama\n",
      "INFO:sagemaker:[INFO ] ModelInfo - M-0001: Apply per model settings:\n",
      "    job_queue_size: 1000\n",
      "    max_dynamic_batch_size: 1\n",
      "    max_batch_delay: 100\n",
      "    max_idle_time: 60\n",
      "    load_on_devices: *\n",
      "    engine: Python\n",
      "    mpi_mode: true\n",
      "    option.entryPoint: null\n",
      "    option.tensor_parallel_degree: 8\n",
      "    option.speculative_draft_model: /opt/ml/additional-model-data-sources/draft_model\n",
      "    option.max_rolling_batch_size: 256\n",
      "    option.mpi_mode: true\n",
      "    option.rolling_batch: lmi-dist\n",
      "INFO:sagemaker:[INFO ] Platform - Found matching platform from: jar:file:/usr/local/djl-serving-0.28.0/lib/python-0.28.0.jar!/native/lib/python.properties\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python_engine.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/arg_parser.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/aws/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/aws/cloud_watch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/chat_completions/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/chat_completions/chat_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/chat_completions/chat_utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/encode_decode.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/huggingface.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/inputs.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/neuron_utils/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/neuron_utils/model_loader.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/neuron_utils/utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/np_util.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/output_formatter.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/outputs.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/pair_list.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/README.md to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/hf_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/lmi_dist_rb_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/scheduler_rb_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/sd_inf2_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/tnx_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/trt_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/vllm_rb_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/request.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/request_io.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/lmi_dist_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/neuron_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/rolling_batch_vllm_utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/scheduler_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/trtllm_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/vllm_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/sagemaker.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/lm_block.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/search_config.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/seq_batch_scheduler.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/seq_batcher.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/seq_batcher_impl.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/step_generation.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/service_loader.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/session_manager.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/sm_log_filter.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/stable_diffusion_inf2.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/streaming_utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/telemetry.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/tensorrt_llm.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/tensorrt_llm_python.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/test_model.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/optimum_modeling.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/optimum_neuron_scheduler.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/slot.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/speculation.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/token_selector.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/ts_service_loader.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] ModelManager - Loading model on Python:[0]\n",
      "INFO:sagemaker:[INFO ] WorkerPool - loading model model (M-0001, PENDING) on gpu(0) ...\n",
      "INFO:sagemaker:[INFO ] ModelInfo - M-0001: Available CPU memory: 1133441 MB, required: 0 MB, reserved: 500 MB\n",
      "INFO:sagemaker:[INFO ] ModelInfo - M-0001: Available GPU memory: 39916 MB, required: 0 MB, reserved: 500 MB\n",
      "INFO:sagemaker:[INFO ] ModelInfo - Loading model model M-0001 on gpu(0)\n",
      "INFO:sagemaker:[INFO ] PyModel - Loading model in MPI mode with TP: 8.\n",
      "INFO:sagemaker:[INFO ] PyModel - Start 1 mpiWorkers ...\n",
      "INFO:sagemaker:[INFO ] PyProcess - Start process: 19000 - retry: 0\n",
      "INFO:sagemaker:[INFO ] Connection - Set CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:305 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:306 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:307 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:300 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:301 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:302 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:303 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,4]<stdout>:304 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,4]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,4]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,4]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:INFO 07-21 02:12:53 arg_utils.py:22] Found draft_model parameter: /opt/ml/additional-model-data-sources/draft_model, will apply speculative patch\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:INFO 07-21 02:12:54 utils.py:660] Found nccl from library /opt/djl/vllm/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:INFO 07-21 02:12:54 vllm_engine.py:68] Using speculative decoding for LLM engine: draft_model='/opt/ml/additional-model-data-sources/draft_model', speculate_length=5, draft_model_tp_size=1, quantization=None\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:INFO 07-21 02:12:54 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,6]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,2]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,7]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,4]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,1]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,5]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,3]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,0]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,0]<stderr>:[rank0]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,4]<stderr>:[rank4]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,1]<stderr>:[rank1]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,2]<stderr>:[rank2]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,3]<stderr>:[rank3]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,6]<stderr>:[rank6]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,5]<stderr>:[rank5]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,7]<stderr>:[rank7]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,0]<stderr>:[rank0]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,1]<stderr>:[rank1]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,0]<stderr>:[rank0]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,2]<stderr>:[rank2]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,1]<stderr>:[rank1]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,6]<stderr>:[rank6]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,3]<stderr>:[rank3]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,4]<stderr>:[rank4]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:INFO 07-21 02:12:56 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,5]<stderr>:[rank5]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,2]<stderr>:[rank2]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,7]<stderr>:[rank7]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,6]<stderr>:[rank6]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,4]<stderr>:[rank4]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,3]<stderr>:[rank3]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,5]<stderr>:[rank5]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,7]<stderr>:[rank7]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:INFO 07-21 02:13:07 utils.py:118] generating GPU P2P access cache for in /opt/djl/vllm/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:INFO 07-21 02:13:09 utils.py:132] reading GPU P2P access cache from /opt/djl/vllm/.config/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:INFO 07-21 02:13:28 model_runner.py:175] Loading model weights took 16.4312 GB\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,4]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,4]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:INFO 07-21 02:13:35 model_runner.py:35] Loading draft model weights took 14.9575 GB\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,5]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,0]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,3]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,1]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,2]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,6]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,7]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,5]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,0]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,3]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,1]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,2]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,6]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-294-model-stderr: [1,7]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:INFO 07-21 02:13:39 vllm_engine.py:62] # GPU blocks: 0, # CPU blocks: 1560\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:Failed invoke service.invoke_handler()\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:Traceback (most recent call last):\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python_engine.py\", line 121, in run_server\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:    outputs = self.service.invoke_handler(function_name, inputs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/service_loader.py\", line 29, in invoke_handler\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:    return getattr(self.module, function_name)(inputs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/huggingface.py\", line 593, in handle\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:    _service.initialize(inputs.get_properties())\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/huggingface.py\", line 152, in initialize\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:    self.rolling_batch = _rolling_batch_cls(\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/rolling_batch/lmi_dist_rolling_batch.py\", line 88, in __init__\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:    self.engine = engine_from_args(engine_args, **kwargs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/init_engine.py\", line 8, in engine_from_args\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:    else:import lmi_dist.vllm_engine;return lmi_dist.vllm_engine.vllm_engine_from_args(A)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/patch/speculative/vllm_engine.py\", line 70, in vllm_engine_from_args\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:    else:init_cache(E,C,B,D)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/vllm_engine.py\", line 62, in init_cache\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:    D=min(A[0]for A in A);E=min(A[1]for A in A);logger.info(f\"# GPU blocks: {D}, # CPU blocks: {E}\");B.initialize_cache(D,E)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/worker/worker.py\", line 17, in initialize_cache\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:    if B<=0:raise ValueError('No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.')\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:Failed invoke service.invoke_handler()\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:Traceback (most recent call last):\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python_engine.py\", line 121, in run_server\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:    outputs = self.service.invoke_handler(function_name, inputs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/service_loader.py\", line 29, in invoke_handler\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:    return getattr(self.module, function_name)(inputs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/huggingface.py\", line 593, in handle\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:    _service.initialize(inputs.get_properties())\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/huggingface.py\", line 152, in initialize\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:    self.rolling_batch = _rolling_batch_cls(\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/rolling_batch/lmi_dist_rolling_batch.py\", line 88, in __init__\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:    self.engine = engine_from_args(engine_args, **kwargs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/init_engine.py\", line 8, in engine_from_args\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:    else:import lmi_dist.vllm_engine;return lmi_dist.vllm_engine.vllm_engine_from_args(A)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/patch/speculative/vllm_engine.py\", line 70, in vllm_engine_from_args\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:    else:init_cache(E,C,B,D)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/vllm_engine.py\", line 62, in init_cache\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:    D=min(A[0]for A in A);E=min(A[1]for A in A);logger.info(f\"# GPU blocks: {D}, # CPU blocks: {E}\");B.initialize_cache(D,E)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/worker/worker.py\", line 17, in initialize_cache\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:    if B<=0:raise ValueError('No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.')\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,1]<stdout>:ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:Failed invoke service.invoke_handler()\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:Traceback (most recent call last):\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python_engine.py\", line 121, in run_server\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:    outputs = self.service.invoke_handler(function_name, inputs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/service_loader.py\", line 29, in invoke_handler\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:    return getattr(self.module, function_name)(inputs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/huggingface.py\", line 593, in handle\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:    _service.initialize(inputs.get_properties())\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/huggingface.py\", line 152, in initialize\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:    self.rolling_batch = _rolling_batch_cls(\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/rolling_batch/lmi_dist_rolling_batch.py\", line 88, in __init__\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:    self.engine = engine_from_args(engine_args, **kwargs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/init_engine.py\", line 8, in engine_from_args\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:    else:import lmi_dist.vllm_engine;return lmi_dist.vllm_engine.vllm_engine_from_args(A)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/patch/speculative/vllm_engine.py\", line 70, in vllm_engine_from_args\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:    else:init_cache(E,C,B,D)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/vllm_engine.py\", line 62, in init_cache\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:    D=min(A[0]for A in A);E=min(A[1]for A in A);logger.info(f\"# GPU blocks: {D}, # CPU blocks: {E}\");B.initialize_cache(D,E)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/worker/worker.py\", line 17, in initialize_cache\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:    if B<=0:raise ValueError('No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.')\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,3]<stdout>:ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:Failed invoke service.invoke_handler()\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:Traceback (most recent call last):\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python_engine.py\", line 121, in run_server\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:    outputs = self.service.invoke_handler(function_name, inputs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/service_loader.py\", line 29, in invoke_handler\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:    return getattr(self.module, function_name)(inputs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/huggingface.py\", line 593, in handle\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:    _service.initialize(inputs.get_properties())\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/huggingface.py\", line 152, in initialize\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:    self.rolling_batch = _rolling_batch_cls(\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/rolling_batch/lmi_dist_rolling_batch.py\", line 88, in __init__\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:    self.engine = engine_from_args(engine_args, **kwargs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/init_engine.py\", line 8, in engine_from_args\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:    else:import lmi_dist.vllm_engine;return lmi_dist.vllm_engine.vllm_engine_from_args(A)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/patch/speculative/vllm_engine.py\", line 70, in vllm_engine_from_args\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:    else:init_cache(E,C,B,D)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/vllm_engine.py\", line 62, in init_cache\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:    D=min(A[0]for A in A);E=min(A[1]for A in A);logger.info(f\"# GPU blocks: {D}, # CPU blocks: {E}\");B.initialize_cache(D,E)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/worker/worker.py\", line 17, in initialize_cache\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:    if B<=0:raise ValueError('No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.')\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,5]<stdout>:ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:Failed invoke service.invoke_handler()\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:Traceback (most recent call last):\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python_engine.py\", line 121, in run_server\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:    outputs = self.service.invoke_handler(function_name, inputs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/service_loader.py\", line 29, in invoke_handler\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:    return getattr(self.module, function_name)(inputs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/huggingface.py\", line 593, in handle\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:    _service.initialize(inputs.get_properties())\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/huggingface.py\", line 152, in initialize\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:    self.rolling_batch = _rolling_batch_cls(\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/rolling_batch/lmi_dist_rolling_batch.py\", line 88, in __init__\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:    self.engine = engine_from_args(engine_args, **kwargs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/init_engine.py\", line 8, in engine_from_args\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:    else:import lmi_dist.vllm_engine;return lmi_dist.vllm_engine.vllm_engine_from_args(A)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/patch/speculative/vllm_engine.py\", line 70, in vllm_engine_from_args\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:    else:init_cache(E,C,B,D)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/vllm_engine.py\", line 62, in init_cache\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:    D=min(A[0]for A in A);E=min(A[1]for A in A);logger.info(f\"# GPU blocks: {D}, # CPU blocks: {E}\");B.initialize_cache(D,E)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/worker/worker.py\", line 17, in initialize_cache\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:    if B<=0:raise ValueError('No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.')\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,7]<stdout>:ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:Failed invoke service.invoke_handler()\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:Traceback (most recent call last):\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python_engine.py\", line 121, in run_server\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:    outputs = self.service.invoke_handler(function_name, inputs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/service_loader.py\", line 29, in invoke_handler\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:    return getattr(self.module, function_name)(inputs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/huggingface.py\", line 593, in handle\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:    _service.initialize(inputs.get_properties())\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/huggingface.py\", line 152, in initialize\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:    self.rolling_batch = _rolling_batch_cls(\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/rolling_batch/lmi_dist_rolling_batch.py\", line 88, in __init__\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:    self.engine = engine_from_args(engine_args, **kwargs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/init_engine.py\", line 8, in engine_from_args\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:    else:import lmi_dist.vllm_engine;return lmi_dist.vllm_engine.vllm_engine_from_args(A)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/patch/speculative/vllm_engine.py\", line 70, in vllm_engine_from_args\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:    else:init_cache(E,C,B,D)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/vllm_engine.py\", line 62, in init_cache\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:    D=min(A[0]for A in A);E=min(A[1]for A in A);logger.info(f\"# GPU blocks: {D}, # CPU blocks: {E}\");B.initialize_cache(D,E)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/worker/worker.py\", line 17, in initialize_cache\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:    if B<=0:raise ValueError('No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.')\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:Failed invoke service.invoke_handler()\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:Traceback (most recent call last):\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python_engine.py\", line 121, in run_server\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:    outputs = self.service.invoke_handler(function_name, inputs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/service_loader.py\", line 29, in invoke_handler\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:    return getattr(self.module, function_name)(inputs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/huggingface.py\", line 593, in handle\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:    _service.initialize(inputs.get_properties())\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/huggingface.py\", line 152, in initialize\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:    self.rolling_batch = _rolling_batch_cls(\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/rolling_batch/lmi_dist_rolling_batch.py\", line 88, in __init__\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:    self.engine = engine_from_args(engine_args, **kwargs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/init_engine.py\", line 8, in engine_from_args\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:    else:import lmi_dist.vllm_engine;return lmi_dist.vllm_engine.vllm_engine_from_args(A)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/patch/speculative/vllm_engine.py\", line 70, in vllm_engine_from_args\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:    else:init_cache(E,C,B,D)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/vllm_engine.py\", line 62, in init_cache\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:    D=min(A[0]for A in A);E=min(A[1]for A in A);logger.info(f\"# GPU blocks: {D}, # CPU blocks: {E}\");B.initialize_cache(D,E)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:  File \"/usr/local/lib/python3.10/dist-packages/lmi_dist/worker/worker.py\", line 17, in initialize_cache\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:    if B<=0:raise ValueError('No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.')\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,6]<stdout>:ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,4]<stdout>:Failed invoke service.invoke_handler()\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,4]<stdout>:Traceback (most recent call last):\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,4]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python_engine.py\", line 121, in run_server\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,4]<stdout>:    outputs = self.service.invoke_handler(function_name, inputs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,4]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/service_loader.py\", line 29, in invoke_handler\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,4]<stdout>:    return getattr(self.module, function_name)(inputs)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:ValueError: Connection disconnected\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,0]<stdout>:300 - Python process finished\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:Python engine process died\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:Traceback (most recent call last):\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python_engine.py\", line 166, in main\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:    engine.run_server()\n",
      "INFO:sagemaker:[INFO ] PyProcess - Stop process: 0:294, failure=true\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python_engine.py\", line 110, in run_server\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:    inputs.read(cl_socket)\n",
      "INFO:sagemaker:[INFO ] PyProcess - Failure count: 0\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/inputs.py\", line 221, in read\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:    prop_size = retrieve_short(conn)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/inputs.py\", line 60, in retrieve_short\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:    data = retrieve_buffer(conn, 2)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:  File \"/tmp/.djl.ai/python/0.28.0/djl_python/inputs.py\", line 36, in retrieve_buffer\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:    raise ValueError(\"Connection disconnected\")\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:ValueError: Connection disconnected\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-294-model-stdout: [1,2]<stdout>:302 - Python process finished\n",
      "INFO:sagemaker:[INFO ] PyProcess - ReaderThread(0) stopped - W-294-model-stdout\n",
      "INFO:sagemaker:[INFO ] PyProcess - ReaderThread(0) stopped - W-294-model-stderr\n",
      "INFO:sagemaker:[ERROR] ModelServer - Failed register workflow\n",
      "INFO:sagemaker:java.util.concurrent.CompletionException: ai.djl.engine.EngineException: Failed to initialize model: prediction failure\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:315) ~[?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:320) [?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1770) [?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1760) [?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373) [?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182) [?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655) [?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622) [?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165) [?:?]\n",
      "INFO:sagemaker:Caused by: ai.djl.engine.EngineException: Failed to initialize model: prediction failure\n",
      "INFO:sagemaker:#011at ai.djl.python.engine.PyProcess.predict(PyProcess.java:111) ~[python-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.python.engine.PyProcess.startPythonProcess(PyProcess.java:170) ~[python-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.python.engine.PyModel.createAllPyProcesses(PyModel.java:314) ~[python-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.python.engine.PyModel.load(PyModel.java:234) ~[python-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.repository.zoo.BaseModelLoader.loadModel(BaseModelLoader.java:166) ~[api-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.repository.zoo.Criteria.loadModel(Criteria.java:174) ~[api-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.serving.wlm.ModelInfo.load(ModelInfo.java:264) ~[wlm-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.serving.wlm.WorkerPool.initWorkers(WorkerPool.java:196) ~[wlm-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.serving.wlm.WorkerPool.initWorkers(WorkerPool.java:179) ~[wlm-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.serving.models.ModelManager.initWorkers(ModelManager.java:233) ~[serving-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.serving.models.ModelManager.lambda$registerWorkflow$2(ModelManager.java:147) ~[serving-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768) ~[?:?]\n",
      "INFO:sagemaker:#011... 6 more\n",
      "INFO:sagemaker:[INFO ] ModelServer - Model server stopped.\n",
      "INFO:sagemaker:[ERROR] ModelServer - Unexpected error\n",
      "INFO:sagemaker:ai.djl.serving.http.ServerStartupException: Failed to initialize startup models and workflows\n",
      "INFO:sagemaker:#011at ai.djl.serving.ModelServer.start(ModelServer.java:210) ~[serving-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.serving.ModelServer.startAndWait(ModelServer.java:174) ~[serving-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.serving.ModelServer.main(ModelServer.java:143) [serving-0.28.0.jar:?]\n",
      "INFO:sagemaker:Caused by: java.util.concurrent.CompletionException: ai.djl.engine.EngineException: Failed to initialize model: prediction failure\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:315) ~[?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:320) ~[?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1770) ~[?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1760) ~[?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373) ~[?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182) ~[?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655) ~[?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622) ~[?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165) ~[?:?]\n",
      "INFO:sagemaker:Caused by: ai.djl.engine.EngineException: Failed to initialize model: prediction failure\n",
      "INFO:sagemaker:#011at ai.djl.python.engine.PyProcess.predict(PyProcess.java:111) ~[python-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.python.engine.PyProcess.startPythonProcess(PyProcess.java:170) ~[python-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.python.engine.PyModel.createAllPyProcesses(PyModel.java:314) ~[python-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.python.engine.PyModel.load(PyModel.java:234) ~[python-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.repository.zoo.BaseModelLoader.loadModel(BaseModelLoader.java:166) ~[api-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.repository.zoo.Criteria.loadModel(Criteria.java:174) ~[api-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.serving.wlm.ModelInfo.load(ModelInfo.java:264) ~[wlm-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.serving.wlm.WorkerPool.initWorkers(WorkerPool.java:196) ~[wlm-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.serving.wlm.WorkerPool.initWorkers(WorkerPool.java:179) ~[wlm-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.serving.models.ModelManager.initWorkers(ModelManager.java:233) ~[serving-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at ai.djl.serving.models.ModelManager.lambda$registerWorkflow$2(ModelManager.java:147) ~[serving-0.28.0.jar:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768) ~[?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1760) ~[?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373) ~[?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182) ~[?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655) ~[?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622) ~[?:?]\n",
      "INFO:sagemaker:#011at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165) ~[?:?]\n",
      "INFO:sagemaker:CUDA compat package requires Nvidia driver ⩽550.90.07\n",
      "INFO:sagemaker:Current installed Nvidia driver version is 535.183.01\n",
      "INFO:sagemaker:Setup CUDA compatibility libs path to LD_LIBRARY_PATH\n",
      "INFO:sagemaker:/usr/local/cuda/compat:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "INFO:sagemaker:[INFO ] ModelServer - Starting model server ...\n",
      "INFO:sagemaker:[INFO ] ModelServer - Starting djl-serving: 0.28.0 ...\n",
      "INFO:sagemaker:[INFO ] ModelServer - \n",
      "INFO:sagemaker:Model server home: /opt/djl\n",
      "INFO:sagemaker:Current directory: /opt/djl\n",
      "INFO:sagemaker:Temp directory: /tmp\n",
      "INFO:sagemaker:Command line: -Dlog4j.configurationFile=/usr/local/djl-serving-0.28.0/conf/log4j2-plain.xml -Xmx1g -Xms1g -XX:+ExitOnOutOfMemoryError -Dai.djl.util.cuda.fork=true -XX:-UseContainerSupport\n",
      "INFO:sagemaker:Number of CPUs: 96\n",
      "INFO:sagemaker:CUDA version: 124 / 80\n",
      "INFO:sagemaker:Number of GPUs: 8\n",
      "INFO:sagemaker:Max heap size: 1024\n",
      "INFO:sagemaker:Config file: /opt/djl/conf/config.properties\n",
      "INFO:sagemaker:Inference address: http://0.0.0.0:8080\n",
      "INFO:sagemaker:Management address: http://0.0.0.0:8080\n",
      "INFO:sagemaker:Default job_queue_size: 1000\n",
      "INFO:sagemaker:Default batch_size: 1\n",
      "INFO:sagemaker:Default max_batch_delay: 100\n",
      "INFO:sagemaker:Default max_idle_time: 60\n",
      "INFO:sagemaker:Model Store: /opt/ml/model\n",
      "INFO:sagemaker:Initial Models: ALL\n",
      "INFO:sagemaker:Netty threads: 0\n",
      "INFO:sagemaker:Maximum Request Size: 67108864\n",
      "INFO:sagemaker:Environment variables:\n",
      "    HF_HUB_ENABLE_HF_TRANSFER: 1\n",
      "    HF_HOME: /tmp/.cache/huggingface\n",
      "    OPTION_SPECULATIVE_DRAFT_MODEL: /opt/ml/additional-model-data-sources/draft_model\n",
      "    OMP_NUM_THREADS: 1\n",
      "    SAGEMAKER_SAFE_PORT_RANGE: 14000-14999\n",
      "    HF_MODEL_ID: /opt/ml/model\n",
      "    SERVING_FEATURES: vllm,lmi-dist\n",
      "    DJL_CACHE_DIR: /tmp/.djl.ai\n",
      "    SAGEMAKER_MODEL_SERVER_WORKERS: 1\n",
      "    SAGEMAKER_ENV: 1\n",
      "    SAGEMAKER_PROGRAM: inference.py\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - scanning for plugins...\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - scanning in plug-in folder :/opt/djl/plugins\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - scanning in plug-in folder :/usr/local/djl-serving-0.28.0/plugins\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: kserve/jar:file:/usr/local/djl-serving-0.28.0/plugins/kserve-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: cache-engines/jar:file:/usr/local/djl-serving-0.28.0/plugins/cache-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: secure-mode/jar:file:/usr/local/djl-serving-0.28.0/plugins/secure-mode-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: static-file-plugin/jar:file:/usr/local/djl-serving-0.28.0/plugins/static-file-plugin-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: console/jar:file:/usr/local/djl-serving-0.28.0/plugins/management-console-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {console/jar:file:/usr/local/djl-serving-0.28.0/plugins/management-console-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin console changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {static-file-plugin/jar:file:/usr/local/djl-serving-0.28.0/plugins/static-file-plugin-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin static-file-plugin changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {cache-engines/jar:file:/usr/local/djl-serving-0.28.0/plugins/cache-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin cache-engines changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {secure-mode/jar:file:/usr/local/djl-serving-0.28.0/plugins/secure-mode-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin secure-mode changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {kserve/jar:file:/usr/local/djl-serving-0.28.0/plugins/kserve-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin kserve changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin console changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin static-file-plugin changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin cache-engines changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin secure-mode changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin kserve changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - 5 plug-ins found and loaded.\n",
      "INFO:sagemaker:[INFO ] ModelServer - Found model model=file:/opt/ml/model/\n",
      "INFO:sagemaker:[INFO ] ModelServer - Found model model=file:/opt/ml/model/\n",
      "INFO:sagemaker:[INFO ] ModelServer - Initializing model: model=file:/opt/ml/model/\n",
      "INFO:sagemaker:[INFO ] LmiUtils - Detected mpi_mode: true, rolling_batch: lmi-dist, tensor_parallel_degree 8, for modelType: llama\n",
      "INFO:sagemaker:[INFO ] ModelInfo - M-0001: Apply per model settings:\n",
      "    job_queue_size: 1000\n",
      "    max_dynamic_batch_size: 1\n",
      "    max_batch_delay: 100\n",
      "    max_idle_time: 60\n",
      "    load_on_devices: *\n",
      "    engine: Python\n",
      "    mpi_mode: true\n",
      "    option.entryPoint: null\n",
      "    option.tensor_parallel_degree: 8\n",
      "    option.speculative_draft_model: /opt/ml/additional-model-data-sources/draft_model\n",
      "    option.max_rolling_batch_size: 256\n",
      "    option.mpi_mode: true\n",
      "    option.rolling_batch: lmi-dist\n",
      "INFO:sagemaker:[INFO ] Platform - Found matching platform from: jar:file:/usr/local/djl-serving-0.28.0/lib/python-0.28.0.jar!/native/lib/python.properties\n",
      "INFO:sagemaker:[INFO ] ModelManager - Loading model on Python:[0]\n",
      "INFO:sagemaker:[INFO ] WorkerPool - loading model model (M-0001, PENDING) on gpu(0) ...\n",
      "INFO:sagemaker:[INFO ] ModelInfo - M-0001: Available CPU memory: 1133447 MB, required: 0 MB, reserved: 500 MB\n",
      "INFO:sagemaker:[INFO ] ModelInfo - M-0001: Available GPU memory: 39916 MB, required: 0 MB, reserved: 500 MB\n",
      "INFO:sagemaker:[INFO ] ModelInfo - Loading model model M-0001 on gpu(0)\n",
      "INFO:sagemaker:[INFO ] PyModel - Loading model in MPI mode with TP: 8.\n",
      "INFO:sagemaker:[INFO ] PyModel - Start 1 mpiWorkers ...\n",
      "INFO:sagemaker:[INFO ] PyProcess - Start process: 19000 - retry: 0\n",
      "INFO:sagemaker:[INFO ] Connection - Set CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,0]<stdout>:256 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,1]<stdout>:257 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,2]<stdout>:258 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,3]<stdout>:259 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,4]<stdout>:260 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,5]<stdout>:261 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,6]<stdout>:262 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,7]<stdout>:263 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '8']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,0]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,2]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,3]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,4]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,5]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,6]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,7]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,1]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,3]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,2]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,6]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,4]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,0]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,1]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,7]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,5]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,0]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,1]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,2]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,3]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,4]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,5]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,6]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,7]<stdout>:Using 8 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,0]<stdout>:INFO 07-21 02:14:05 arg_utils.py:22] Found draft_model parameter: /opt/ml/additional-model-data-sources/draft_model, will apply speculative patch\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,0]<stdout>:INFO 07-21 02:14:05 utils.py:660] Found nccl from library /opt/djl/vllm/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,0]<stdout>:INFO 07-21 02:14:05 vllm_engine.py:68] Using speculative decoding for LLM engine: draft_model='/opt/ml/additional-model-data-sources/draft_model', speculate_length=5, draft_model_tp_size=1, quantization=None\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,0]<stdout>:INFO 07-21 02:14:05 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,7]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,5]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,3]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,6]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,4]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,2]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,1]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,0]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,0]<stderr>:[rank0]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,3]<stderr>:[rank3]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,1]<stderr>:[rank1]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,4]<stderr>:[rank4]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,5]<stderr>:[rank5]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,2]<stderr>:[rank2]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,6]<stderr>:[rank6]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,7]<stderr>:[rank7]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,0]<stderr>:[rank0]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,3]<stderr>:[rank3]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,0]<stderr>:[rank0]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,0]<stdout>:INFO 07-21 02:14:07 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,1]<stderr>:[rank1]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,2]<stderr>:[rank2]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,4]<stderr>:[rank4]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,1]<stderr>:[rank1]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,5]<stderr>:[rank5]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,3]<stderr>:[rank3]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,6]<stderr>:[rank6]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,7]<stderr>:[rank7]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,2]<stderr>:[rank2]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,4]<stderr>:[rank4]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,5]<stderr>:[rank5]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,6]<stderr>:[rank6]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,7]<stderr>:[rank7]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,0]<stdout>:INFO 07-21 02:14:57 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,0]<stdout>:INFO 07-21 02:15:08 custom_all_reduce.py:246] Registering 5635 cuda graph addresses\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,0]<stdout>:INFO 07-21 02:15:08 model_runner.py:1017] Graph capturing finished in 11 secs.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-250-model-stdout: [1,0]<stdout>:INFO 07-21 02:15:08 worker.py:38] CUDA graphs took took 0.4473 (speculation) + 0.4004 (fallback) = 0.8477 GB\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,4]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,2]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,3]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,5]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,6]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,0]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,1]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-250-model-stderr: [1,7]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[INFO ] PyProcess - Model [model] initialized.\n",
      "INFO:sagemaker:[INFO ] PyModel - model model loaded in 71089 ms.\n",
      "INFO:sagemaker:[INFO ] WorkerPool - scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "INFO:sagemaker:[INFO ] WorkerThread - Starting worker thread WT-0001 for model model (M-0001, READY) on device gpu(0)\n",
      "INFO:sagemaker:[INFO ] ModelServer - Initialize BOTH server with: EpollServerSocketChannel.\n",
      "INFO:sagemaker:[INFO ] ModelServer - BOTH API bind to: http://0.0.0.0:8080\n",
      "INFO:sagemaker:Created endpoint with name meta-textgeneration-llama-3-70b-2024-07-21-02-06-39-418\n",
      "ModelBuilder: DEBUG:     ModelBuilder metrics emitted.\n"
     ]
    }
   ],
   "source": [
    "predictor = optimized_model.deploy(accept_eula=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4edf3-11d1-4861-99d5-44667d70d061",
   "metadata": {},
   "source": [
    "Once the deployment has finished successfully, you can send queries to the model by simply using the predictor's `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c198a59-c6a6-4ec6-ad49-f838798d7071",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': \" and I'm here to help you with your questions about the world of technology. Today, I want to talk about the importance of having a strong online presence for your business. In today's digital age, having a website is no longer a luxury, but a necessity. It's the first place potential customers will go to learn more about your business, and it's essential to make a good first impression.\\nBut having a website is just the first step. You also need to make sure that your website is optimized for search engines, so that it can be easily found by potential customers. This is where search engine optimization (SEO) comes in\"}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe61ec19-05f2-47fe-a0ab-e45bf3cdf457",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: meta-textgeneration-llama-3-70b-2024-07-21-02-06-38-904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: meta-textgeneration-llama-3-70b-2024-07-21-02-06-39-418\n",
      "INFO:sagemaker:Deleting endpoint with name: meta-textgeneration-llama-3-70b-2024-07-21-02-06-39-418\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5ff522-c231-4f52-ac36-0a56e1e446d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Run optimization job to quantize the model using AWQ, then deploy the quantized model\n",
    "In this section, you will quantize the `meta-textgeneration-llama-3-70b` JumpStart model with the AWQ quantization algorithm by running an Amazon SageMaker optimization job. \n",
    "\n",
    "### What is quantization?\n",
    "In our particular context, quantization means casting the weights of a pre-trained LLM to a data type with a lower number of bits and therefore a smaller memory footprint. The benefits of LLM quantization include:\n",
    "* Reduced hardware requirements for model serving: A quantized model can be served using less expensive and more available GPUs or even made accessible on consumer devices or mobile platforms.\n",
    "* Increased space for the KV cache to enable larger batch sizes and/or sequence lengths.\n",
    "* Faster decoding latency. As the decoding process is memory bandwidth bound, less data movement from reduced weight sizes directly improves decoding latency, unless offset by dequantization overhead.\n",
    "* A higher compute-to-memory access ratio (through reduced data movement), known as arithmetic intensity. This allows for fuller utilization of available compute resources during decoding.  \n",
    "\n",
    "AWQ (Activation-aware Weight Quantization) is a post-training weight-only quantization algorithm introduced by [J. Lin et al. (MLSys 2024)](https://arxiv.org/abs/2306.00978) that allows to quantize LLMs to low-bit integer types like INT4 with virtually no loss in model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ded24c7d-5d4c-4a9d-acfb-3accb4e544d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_builder = ModelBuilder(\n",
    "    model=js_model_id,\n",
    "    schema_builder=schema_builder,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role_arn=execution_role_arn,\n",
    "    log_level=logging.ERROR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b235b250-fda4-4c4a-b149-d596ed7d706e",
   "metadata": {},
   "source": [
    "Quantizing the model is as easy as supplying the following inputs:\n",
    "* The location of the unquantized model artifacts, here the Amazon SageMaker JumpStart model ID.\n",
    "* The quantization configuration.\n",
    "* The Amazon S3 URI where the output quantized artifacts needs to be stored.\n",
    "Everything else (compute provisioning and configuration for example) is managed by SageMaker. This operation takes around 120min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dba22999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ml.p4d.24xlarge'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_instance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e379f0d8-e8e4-42d0-99ec-994b9c53eba6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "INFO:sagemaker.jumpstart:Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "WARNING:sagemaker.jumpstart:Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "ModelBuilder: INFO:     JumpStart Model ID detected.\n",
      "ModelBuilder: INFO:     Either inference spec or model is provided. ModelBuilder is not handling MLflow model input\n",
      "ModelBuilder: INFO:     JumpStart Model ID detected.\n",
      "ModelBuilder: INFO:     JumpStart Model ID detected.\n",
      "Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "INFO:sagemaker.jumpstart:Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "WARNING:sagemaker.jumpstart:Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "No instance type selected for inference hosting endpoint. Defaulting to ml.p4d.24xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.p4d.24xlarge.\n",
      "ModelBuilder: INFO:     JumpStart ID meta-textgeneration-llama-3-70b is packaged with Image URI: 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.28.0-lmi10.0.0-cu124\n",
      "ModelBuilder: INFO:     Building for DJL JumpStart Model ID...\n",
      "ModelBuilder: WARNING:     Unable to check docker volume utilization at the expected path /var/lib/docker. [Errno 2] No such file or directory: '/var/lib/docker'\n",
      "Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "WARNING:sagemaker.jumpstart:Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "WARNING:sagemaker.jumpstart:Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "No instance type selected for inference hosting endpoint. Defaulting to ml.p4d.24xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.p4d.24xlarge.\n",
      "Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "WARNING:sagemaker.jumpstart:Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "WARNING:sagemaker.jumpstart:Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: DEBUG:     ModelBuilder metrics emitted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimized_model = model_builder.optimize(\n",
    "    instance_type=gpu_instance_type,\n",
    "    accept_eula=True,\n",
    "    quantization_config={\n",
    "        \"OverrideEnvironment\": {\n",
    "            \"OPTION_QUANTIZE\": \"awq\",\n",
    "        },\n",
    "    },\n",
    "    output_path=f\"s3://{artifacts_bucket_name}/awq-quantization/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb119673-840e-473d-b44a-1e0b7fb61394",
   "metadata": {},
   "source": [
    "Now let's deploy the quantized model to an Amazon SageMaker endpoint. This operation may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "953c8e0d-be1b-498c-b6c2-19bd35fd45c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-3-70b-2024-07-21-02-15-50-465\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-3-70b-2024-07-21-04-12-18-616\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-3-70b-2024-07-21-04-12-18-616\n",
      "INFO:sagemaker:CUDA compat package requires Nvidia driver ⩽550.90.07\n",
      "INFO:sagemaker:Current installed Nvidia driver version is 535.183.01\n",
      "INFO:sagemaker:Setup CUDA compatibility libs path to LD_LIBRARY_PATH\n",
      "INFO:sagemaker:/usr/local/cuda/compat:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "INFO:sagemaker:[INFO ] ModelServer - Starting model server ...\n",
      "INFO:sagemaker:[INFO ] ModelServer - Starting djl-serving: 0.28.0 ...\n",
      "INFO:sagemaker:[INFO ] ModelServer - \n",
      "INFO:sagemaker:Model server home: /opt/djl\n",
      "INFO:sagemaker:Current directory: /opt/djl\n",
      "INFO:sagemaker:Temp directory: /tmp\n",
      "INFO:sagemaker:Command line: -Dlog4j.configurationFile=/usr/local/djl-serving-0.28.0/conf/log4j2-plain.xml -Xmx1g -Xms1g -XX:+ExitOnOutOfMemoryError -Dai.djl.util.cuda.fork=true -XX:-UseContainerSupport\n",
      "INFO:sagemaker:Number of CPUs: 48\n",
      "INFO:sagemaker:CUDA version: 124 / 86\n",
      "INFO:sagemaker:Number of GPUs: 4\n",
      "INFO:sagemaker:Max heap size: 1024\n",
      "INFO:sagemaker:Config file: /opt/djl/conf/config.properties\n",
      "INFO:sagemaker:Inference address: http://0.0.0.0:8080\n",
      "INFO:sagemaker:Management address: http://0.0.0.0:8080\n",
      "INFO:sagemaker:Default job_queue_size: 1000\n",
      "INFO:sagemaker:Default batch_size: 1\n",
      "INFO:sagemaker:Default max_batch_delay: 100\n",
      "INFO:sagemaker:Default max_idle_time: 60\n",
      "INFO:sagemaker:Model Store: /opt/ml/model\n",
      "INFO:sagemaker:Initial Models: ALL\n",
      "INFO:sagemaker:Netty threads: 0\n",
      "INFO:sagemaker:Maximum Request Size: 67108864\n",
      "INFO:sagemaker:Environment variables:\n",
      "    HF_HUB_ENABLE_HF_TRANSFER: 1\n",
      "    HF_HOME: /tmp/.cache/huggingface\n",
      "    OPTION_QUANTIZE: awq\n",
      "    OMP_NUM_THREADS: 1\n",
      "    SAGEMAKER_SAFE_PORT_RANGE: 24000-24999\n",
      "    HF_MODEL_ID: /opt/ml/model\n",
      "    SERVING_FEATURES: vllm,lmi-dist\n",
      "    DJL_CACHE_DIR: /tmp/.djl.ai\n",
      "    SAGEMAKER_MODEL_SERVER_WORKERS: 1\n",
      "    SAGEMAKER_ENV: 1\n",
      "    SAGEMAKER_PROGRAM: inference.py\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - scanning for plugins...\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - scanning in plug-in folder :/opt/djl/plugins\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - scanning in plug-in folder :/usr/local/djl-serving-0.28.0/plugins\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: console/jar:file:/usr/local/djl-serving-0.28.0/plugins/management-console-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: kserve/jar:file:/usr/local/djl-serving-0.28.0/plugins/kserve-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: secure-mode/jar:file:/usr/local/djl-serving-0.28.0/plugins/secure-mode-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: cache-engines/jar:file:/usr/local/djl-serving-0.28.0/plugins/cache-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: static-file-plugin/jar:file:/usr/local/djl-serving-0.28.0/plugins/static-file-plugin-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {console/jar:file:/usr/local/djl-serving-0.28.0/plugins/management-console-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin console changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {static-file-plugin/jar:file:/usr/local/djl-serving-0.28.0/plugins/static-file-plugin-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin static-file-plugin changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {cache-engines/jar:file:/usr/local/djl-serving-0.28.0/plugins/cache-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin cache-engines changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {secure-mode/jar:file:/usr/local/djl-serving-0.28.0/plugins/secure-mode-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin secure-mode changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {kserve/jar:file:/usr/local/djl-serving-0.28.0/plugins/kserve-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin kserve changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin console changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin static-file-plugin changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin cache-engines changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin secure-mode changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin kserve changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - 5 plug-ins found and loaded.\n",
      "INFO:sagemaker:[INFO ] ModelServer - Found model model=file:/opt/ml/model/\n",
      "INFO:sagemaker:[INFO ] ModelServer - Found model model=file:/opt/ml/model/\n",
      "INFO:sagemaker:[INFO ] ModelServer - Initializing model: model=file:/opt/ml/model/\n",
      "INFO:sagemaker:[INFO ] LmiUtils - Detected mpi_mode: true, rolling_batch: lmi-dist, tensor_parallel_degree 4, for modelType: llama\n",
      "INFO:sagemaker:[INFO ] ModelInfo - M-0001: Apply per model settings:\n",
      "    job_queue_size: 1000\n",
      "    max_dynamic_batch_size: 1\n",
      "    max_batch_delay: 100\n",
      "    max_idle_time: 60\n",
      "    load_on_devices: *\n",
      "    engine: Python\n",
      "    mpi_mode: true\n",
      "    option.entryPoint: null\n",
      "    option.parallel_loading: True\n",
      "    option.tensor_parallel_degree: 4\n",
      "    option.max_rolling_batch_size: 256\n",
      "    option.quantize: awq\n",
      "    option.mpi_mode: true\n",
      "    option.rolling_batch: lmi-dist\n",
      "INFO:sagemaker:[INFO ] Platform - Found matching platform from: jar:file:/usr/local/djl-serving-0.28.0/lib/python-0.28.0.jar!/native/lib/python.properties\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python_engine.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/arg_parser.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/aws/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/aws/cloud_watch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/chat_completions/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/chat_completions/chat_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/chat_completions/chat_utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/encode_decode.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/huggingface.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/inputs.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/neuron_utils/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/neuron_utils/model_loader.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/neuron_utils/utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/np_util.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/output_formatter.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/outputs.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/pair_list.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/README.md to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/hf_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/lmi_dist_rb_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/scheduler_rb_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/sd_inf2_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/tnx_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/trt_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/vllm_rb_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/request.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/request_io.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/lmi_dist_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/neuron_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/rolling_batch_vllm_utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/scheduler_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/trtllm_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/vllm_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/sagemaker.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/lm_block.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/search_config.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/seq_batch_scheduler.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/seq_batcher.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/seq_batcher_impl.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/step_generation.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/service_loader.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/session_manager.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/sm_log_filter.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/stable_diffusion_inf2.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/streaming_utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/telemetry.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/tensorrt_llm.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/tensorrt_llm_python.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/test_model.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/optimum_modeling.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/optimum_neuron_scheduler.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/slot.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/speculation.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/token_selector.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/ts_service_loader.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] ModelManager - Loading model on Python:[0]\n",
      "INFO:sagemaker:[INFO ] WorkerPool - loading model model (M-0001, PENDING) on gpu(0) ...\n",
      "INFO:sagemaker:[INFO ] ModelInfo - M-0001: Available CPU memory: 184276 MB, required: 0 MB, reserved: 500 MB\n",
      "INFO:sagemaker:[INFO ] ModelInfo - M-0001: Available GPU memory: 22259 MB, required: 0 MB, reserved: 500 MB\n",
      "INFO:sagemaker:[INFO ] ModelInfo - Loading model model M-0001 on gpu(0)\n",
      "INFO:sagemaker:[INFO ] PyModel - Loading model in MPI mode with TP: 4.\n",
      "INFO:sagemaker:[INFO ] PyModel - Start 1 mpiWorkers ...\n",
      "INFO:sagemaker:[INFO ] PyProcess - Start process: 19000 - retry: 0\n",
      "INFO:sagemaker:[INFO ] Connection - Set CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,1]<stdout>:299 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '4']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,2]<stdout>:300 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '4']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,0]<stdout>:298 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '4']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,3]<stdout>:301 - djl_python_engine started with args: ['--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.huggingface', '--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--tensor-parallel-degree', '4']\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,1]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,0]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,3]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,2]<stdout>:PyTorch version 2.3.0+cu121 available.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,0]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,2]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,3]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,1]<stdout>:Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,0]<stdout>:Using 4 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,1]<stdout>:Using 4 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,2]<stdout>:Using 4 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,3]<stdout>:Using 4 gpus\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,0]<stdout>:WARNING 07-21 04:18:41 config.py:205] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,0]<stdout>:INFO 07-21 04:18:41 utils.py:660] Found nccl from library /opt/djl/vllm/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,0]<stdout>:INFO 07-21 04:18:41 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,3]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,1]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,2]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,0]<stderr>:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,1]<stderr>:[rank1]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,0]<stderr>:[rank0]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,2]<stderr>:[rank2]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,3]<stderr>:[rank3]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,0]<stderr>:[rank0]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,2]<stderr>:[rank2]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,0]<stdout>:INFO 07-21 04:18:43 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,1]<stderr>:[rank1]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,3]<stderr>:[rank3]:[W Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function getCvarBool)\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,0]<stdout>:WARNING 07-21 04:18:43 custom_all_reduce.py:65] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,0]<stdout>:INFO 07-21 04:18:47 model_runner.py:175] Loading model weights took 9.2842 GB\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,0]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,1]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,2]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,3]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,0]<stdout>:INFO 07-21 04:18:57 vllm_engine.py:62] # GPU blocks: 7056, # CPU blocks: 3276\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,0]<stdout>:INFO 07-21 04:18:59 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,0]<stdout>:INFO 07-21 04:18:59 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-292-model-stdout: [1,0]<stdout>:INFO 07-21 04:19:14 model_runner.py:1017] Graph capturing finished in 15 secs.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,1]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,3]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,0]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-292-model-stderr: [1,2]<stderr>:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:sagemaker:[INFO ] PyProcess - Model [model] initialized.\n",
      "INFO:sagemaker:[INFO ] PyModel - model model loaded in 37926 ms.\n",
      "INFO:sagemaker:[INFO ] WorkerPool - scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "INFO:sagemaker:[INFO ] WorkerThread - Starting worker thread WT-0001 for model model (M-0001, READY) on device gpu(0)\n",
      "INFO:sagemaker:[INFO ] ModelServer - Initialize BOTH server with: EpollServerSocketChannel.\n",
      "INFO:sagemaker:[INFO ] ModelServer - BOTH API bind to: http://0.0.0.0:8080\n",
      "INFO:sagemaker:Created endpoint with name meta-textgeneration-llama-3-70b-2024-07-21-04-12-18-616\n",
      "ModelBuilder: DEBUG:     ModelBuilder metrics emitted.\n"
     ]
    }
   ],
   "source": [
    "quantized_instance_type = \"ml.g5.12xlarge\"  # We can use a smaller instance type once quantized\n",
    "predictor = optimized_model.deploy(instance_type=quantized_instance_type, accept_eula=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab86b58-a250-48df-a024-119aa4290b05",
   "metadata": {},
   "source": [
    "Once the deployment has finished successfully, you can send queries to the model by simply using the predictor's `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f1cd1c5-0bba-4bd2-b489-6d1fdae8388e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': \" and I'm here to help you with your English language needs. Whether you're a student, a professional, or just someone who wants to improve their English skills, I can provide you with the guidance and support you need to succeed.\\nI can help you with a wide range of language-related tasks, including:\\nGrammar: I can help you understand the rules of English grammar and how to use them correctly in your writing and speaking.\\nVocabulary: I can help you expand your vocabulary and learn new words and phrases that will make your writing and speaking more effective.\\nPronunciation: I can help you improve your pronunciation of English words and phrases\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2e73288-312e-48ee-ae87-aa18e2a28bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: meta-textgeneration-llama-3-70b-2024-07-21-02-15-50-465\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: meta-textgeneration-llama-3-70b-2024-07-21-04-12-18-616\n",
      "INFO:sagemaker:Deleting endpoint with name: meta-textgeneration-llama-3-70b-2024-07-21-04-12-18-616\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4550542-4e10-452f-bca7-a1f1a24a28f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Run optimization job to compile the model using the AWS Neuron Compiler then deploy the compiled model to an Inferentia 2 endpoint\n",
    "In this section, you will use an Amazon SageMaker optimization job as a managed model compiler to compile the `meta-textgeneration-llama-3-70b` JumpStart model for [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/) hardware. The optimization job allows you to decouple compilation from deployment. The model is compiled once while the compiled artifacts can be deployed many times. In other words, the compilation overhead is paid once instead of occuring upon each deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75309273-2780-44c7-acfe-4879029e00cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_builder = ModelBuilder(\n",
    "    model=js_model_id,\n",
    "    schema_builder=schema_builder,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role_arn=execution_role_arn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d28124-6dc0-4643-9472-86cc7ab75d05",
   "metadata": {},
   "source": [
    "Now let's compile the model for Inferentia 2. This operation takes around 40min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef26e543-6951-40dd-801a-93984e3ffaf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "INFO:sagemaker.jumpstart:Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "WARNING:sagemaker.jumpstart:Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "ModelBuilder: INFO:     JumpStart Model ID detected.\n",
      "ModelBuilder: INFO:     Either inference spec or model is provided. ModelBuilder is not handling MLflow model input\n",
      "ModelBuilder: INFO:     JumpStart Model ID detected.\n",
      "ModelBuilder: INFO:     JumpStart Model ID detected.\n",
      "Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "INFO:sagemaker.jumpstart:Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "WARNING:sagemaker.jumpstart:Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "No instance type selected for inference hosting endpoint. Defaulting to ml.p4d.24xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.p4d.24xlarge.\n",
      "ModelBuilder: INFO:     JumpStart ID meta-textgeneration-llama-3-70b is packaged with Image URI: 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.28.0-lmi10.0.0-cu124\n",
      "ModelBuilder: INFO:     Building for DJL JumpStart Model ID...\n",
      "ModelBuilder: WARNING:     Unable to check docker volume utilization at the expected path /var/lib/docker. [Errno 2] No such file or directory: '/var/lib/docker'\n",
      "Model 'meta-textgenerationneuron-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "INFO:sagemaker.jumpstart:Model 'meta-textgenerationneuron-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "No instance type selected for inference hosting endpoint. Defaulting to ml.trn1.32xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.trn1.32xlarge.\n",
      "Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "INFO:sagemaker.jumpstart:Model 'meta-textgeneration-llama-3-70b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "WARNING:sagemaker.jumpstart:Using model 'meta-textgeneration-llama-3-70b' with wildcard version identifier '*'. You can pin to version '2.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "WARNING:sagemaker.jumpstart:Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "WARNING:sagemaker.jumpstart:Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "No instance type selected for inference hosting endpoint. Defaulting to ml.p4d.24xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.p4d.24xlarge.\n",
      "Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "WARNING:sagemaker.jumpstart:Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n",
      "WARNING:sagemaker.jumpstart:Instance rate metrics will be omitted. Reason: User: arn:aws:sts::057716757052:assumed-role/new-sagemaker-studio-gonsoomoon/SageMaker is not authorized to perform: pricing:GetProducts because no identity-based policy allows the pricing:GetProducts action\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................................................................................................................................................................................................................................................................................................................................................................................................................................................!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: DEBUG:     ModelBuilder metrics emitted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimized_model = model_builder.optimize(\n",
    "    instance_type=neuron_instance_type,\n",
    "    accept_eula=True,\n",
    "    compilation_config={\n",
    "        \"OverrideEnvironment\": {\n",
    "            \"OPTION_TENSOR_PARALLEL_DEGREE\": \"24\",\n",
    "            \"OPTION_N_POSITIONS\": \"8192\",\n",
    "            \"OPTION_DTYPE\": \"fp16\",\n",
    "            \"OPTION_ROLLING_BATCH\": \"auto\",\n",
    "            \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"4\",\n",
    "            \"OPTION_NEURON_OPTIMIZE_LEVEL\": \"2\",\n",
    "        }\n",
    "    },\n",
    "    output_path=f\"s3://{artifacts_bucket_name}/neuron/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3811e8b3-f145-4d08-85ee-3a8d797d5ae6",
   "metadata": {},
   "source": [
    "Now let's deploy the compiled model to an Amazon SageMaker endpoint powered by Inferentia 2 hardware. This operation may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f5c8835-081d-457f-aabd-53dc948df0e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-3-70b-2024-07-21-04-23-19-484\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-3-70b-2024-07-21-05-03-18-831\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-3-70b-2024-07-21-05-03-18-831\n",
      "INFO:sagemaker:[INFO ] ModelServer - Starting model server ...\n",
      "INFO:sagemaker:[INFO ] ModelServer - Starting djl-serving: 0.28.0 ...\n",
      "INFO:sagemaker:[INFO ] ModelServer - \n",
      "INFO:sagemaker:Model server home: /opt/djl\n",
      "INFO:sagemaker:Current directory: /opt/djl\n",
      "INFO:sagemaker:Temp directory: /tmp\n",
      "INFO:sagemaker:Command line: -Dlog4j.configurationFile=/usr/local/djl-serving-0.28.0/conf/log4j2-plain.xml -Xmx1g -Xms1g -Xss2m -XX:+ExitOnOutOfMemoryError -XX:-UseContainerSupport\n",
      "INFO:sagemaker:Number of CPUs: 192\n",
      "INFO:sagemaker:Number of Neuron cores: 24\n",
      "INFO:sagemaker:Max heap size: 1024\n",
      "INFO:sagemaker:Config file: /opt/djl/conf/config.properties\n",
      "INFO:sagemaker:Inference address: http://0.0.0.0:8080\n",
      "INFO:sagemaker:Management address: http://0.0.0.0:8080\n",
      "INFO:sagemaker:Default job_queue_size: 1000\n",
      "INFO:sagemaker:Default batch_size: 1\n",
      "INFO:sagemaker:Default max_batch_delay: 100\n",
      "INFO:sagemaker:Default max_idle_time: 60\n",
      "INFO:sagemaker:Model Store: /opt/ml/model\n",
      "INFO:sagemaker:Initial Models: ALL\n",
      "INFO:sagemaker:Netty threads: 0\n",
      "INFO:sagemaker:Maximum Request Size: 67108864\n",
      "INFO:sagemaker:Environment variables:\n",
      "    OPTION_ROLLING_BATCH: auto\n",
      "    OPTION_TENSOR_PARALLEL_DEGREE: 24\n",
      "    HF_HOME: /tmp/.cache/huggingface\n",
      "    OPTION_NEURON_OPTIMIZE_LEVEL: 2\n",
      "    OPTION_MAX_ROLLING_BATCH_SIZE: 4\n",
      "    OPTION_N_POSITIONS: 8192\n",
      "    SERVING_FEATURES: vllm,lmi-dist,tnx\n",
      "    DJL_CACHE_DIR: /tmp/.djl.ai\n",
      "    SAGEMAKER_MODEL_SERVER_WORKERS: 1\n",
      "    OMP_NUM_THREADS: 1\n",
      "    SAGEMAKER_ENV: 1\n",
      "    SAGEMAKER_SAFE_PORT_RANGE: 19000-19999\n",
      "    SAGEMAKER_PROGRAM: inference.py\n",
      "    OPTION_DTYPE: fp16\n",
      "    HF_MODEL_ID: /opt/ml/model\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - scanning for plugins...\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - scanning in plug-in folder :/opt/djl/plugins\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - scanning in plug-in folder :/usr/local/djl-serving-0.28.0/plugins\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: console/jar:file:/usr/local/djl-serving-0.28.0/plugins/management-console-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: secure-mode/jar:file:/usr/local/djl-serving-0.28.0/plugins/secure-mode-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: static-file-plugin/jar:file:/usr/local/djl-serving-0.28.0/plugins/static-file-plugin-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: cache-engines/jar:file:/usr/local/djl-serving-0.28.0/plugins/cache-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] PropertyFilePluginMetaDataReader - Plugin found: kserve/jar:file:/usr/local/djl-serving-0.28.0/plugins/kserve-0.28.0.jar!/META-INF/plugin.definition\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {console/jar:file:/usr/local/djl-serving-0.28.0/plugins/management-console-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin console changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {static-file-plugin/jar:file:/usr/local/djl-serving-0.28.0/plugins/static-file-plugin-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin static-file-plugin changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {cache-engines/jar:file:/usr/local/djl-serving-0.28.0/plugins/cache-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin cache-engines changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {secure-mode/jar:file:/usr/local/djl-serving-0.28.0/plugins/secure-mode-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin secure-mode changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - Loading plugin: {kserve/jar:file:/usr/local/djl-serving-0.28.0/plugins/kserve-0.28.0.jar!/META-INF/plugin.definition}\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin kserve changed state to INITIALIZED\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin console changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin static-file-plugin changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin cache-engines changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin secure-mode changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] PluginMetaData - plugin kserve changed state to ACTIVE reason: plugin ready\n",
      "INFO:sagemaker:[INFO ] FolderScanPluginManager - 5 plug-ins found and loaded.\n",
      "INFO:sagemaker:[INFO ] ModelServer - Found model model=file:/opt/ml/model/\n",
      "INFO:sagemaker:[INFO ] ModelServer - Found model model=file:/opt/ml/model/\n",
      "INFO:sagemaker:[INFO ] ModelServer - Initializing model: model=file:/opt/ml/model/\n",
      "INFO:sagemaker:[INFO ] LmiUtils - Detected mpi_mode: null, rolling_batch: tnx, tensor_parallel_degree 24, for modelType: llama\n",
      "INFO:sagemaker:[INFO ] ModelInfo - M-0001: Apply per model settings:\n",
      "    job_queue_size: 1000\n",
      "    max_dynamic_batch_size: 1\n",
      "    max_batch_delay: 100\n",
      "    max_idle_time: 60\n",
      "    load_on_devices: *\n",
      "    engine: Python\n",
      "    mpi_mode: null\n",
      "    option.entryPoint: null\n",
      "    option.parallel_loading: True\n",
      "    option.dtype: fp16\n",
      "    option.n_positions: 8192\n",
      "    option.tensor_parallel_degree: 24\n",
      "    option.neuron_optimize_level: 2\n",
      "    option.max_rolling_batch_size: 4\n",
      "    option.rolling_batch: tnx\n",
      "    option.model_id: ./optimized_model\n",
      "INFO:sagemaker:[INFO ] Platform - Found matching platform from: jar:file:/usr/local/djl-serving-0.28.0/lib/python-0.28.0.jar!/native/lib/python.properties\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python_engine.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/arg_parser.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/aws/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/aws/cloud_watch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/chat_completions/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/chat_completions/chat_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/chat_completions/chat_utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/encode_decode.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/huggingface.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/inputs.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/neuron_utils/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/neuron_utils/model_loader.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/neuron_utils/utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/np_util.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/output_formatter.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/outputs.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/pair_list.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/README.md to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/hf_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/lmi_dist_rb_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/scheduler_rb_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/sd_inf2_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/tnx_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/trt_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/properties_manager/vllm_rb_properties.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/request.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/request_io.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/lmi_dist_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/neuron_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/rolling_batch_vllm_utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/scheduler_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/trtllm_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/rolling_batch/vllm_rolling_batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/sagemaker.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/batch.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/lm_block.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/search_config.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/seq_batch_scheduler.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/seq_batcher.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/seq_batcher_impl.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/step_generation.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/seq_scheduler/utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/service_loader.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/session_manager.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/sm_log_filter.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/stable_diffusion_inf2.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/streaming_utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/tensorrt_llm.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/tensorrt_llm_python.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/test_model.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/__init__.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/optimum_modeling.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/optimum_neuron_scheduler.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/slot.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/speculation.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/token_selector.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/transformers_neuronx_scheduler/utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/ts_service_loader.py to cache ...\n",
      "INFO:sagemaker:[INFO ] PyEnv - Extracting /djl_python/utils.py to cache ...\n",
      "INFO:sagemaker:[INFO ] ModelManager - Loading model on Python:[nc0]\n",
      "INFO:sagemaker:[INFO ] WorkerPool - loading model model (M-0001, PENDING) on nc(0) ...\n",
      "INFO:sagemaker:[INFO ] ModelInfo - M-0001: Available CPU memory: 747286 MB, required: 0 MB, reserved: 500 MB\n",
      "INFO:sagemaker:[INFO ] ModelInfo - Loading model model M-0001 on nc(0)\n",
      "INFO:sagemaker:[INFO ] WorkerPool - scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "INFO:sagemaker:[INFO ] PyProcess - Start process: 19000 - retry: 0\n",
      "INFO:sagemaker:[INFO ] Connection - Set NEURON_RT_VISIBLE_CORES=0-23\n",
      "INFO:sagemaker:[INFO ] Connection - Set OMP_NUM_THREADS=48\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-81-model-stdout: 81 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '0']\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-81-model-stderr: /usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-81-model-stderr:   warnings.warn(\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-81-model-stdout: PJRT_DEVICE not set, defaulting to NEURON\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-81-model-stdout: WARNING 07-21 05:34:23 ray_utils.py:46] Failed to import Ray with ModuleNotFoundError(\"No module named 'ray'\"). For distributed inference, please install Ray with `pip install ray`.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-81-model-stdout: Python engine started.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-81-model-stdout: Model loading properties: model_id_or_path='./optimized_model' rolling_batch=<RollingBatchEnum.tnx: 'tnx'> tensor_parallel_degree=24 trust_remote_code=False enable_streaming=<StreamingEnum.false: 'false'> batch_size=4 max_rolling_batch_size=4 dtype=<Dtype.f16: 'fp16'> revision=None output_formatter=None waiting_steps=None mpi_mode=False tgi_compat=False draft_model_id=None spec_length=0 neuron_optimize_level=None enable_mixed_precision_accumulation=None enable_saturate_infinity=None n_positions=8192 unroll=None load_in_8bit=None low_cpu_mem_usage=False load_split_model=None context_length_estimate=None amp='f16' quantize=None compiled_graph_path=None draft_model_compiled_path=None speculative_draft_model=None speculative_length=5 draft_model_tp_size=None task=None save_mp_checkpoint_path=None group_query_attention=None model_loader=None rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'> fuse_qkv=None attention_layout=None collectives_layout=None cache_layout=None partition_schema=None all_reduce_dtype=None cast_logits_dtype=None on_device_embedding_config={}\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-81-model-stdout: Start loading the model ./optimized_model using NeuronAutoModel...\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-81-model-stderr: /usr/local/lib/python3.10/dist-packages/transformers_neuronx/decoder.py:154: UserWarning: KV head replication will be enabled since the number of KV heads (8) is not evenly divisible by the tensor parallel degree (24)\n",
      "INFO:sagemaker:[WARN ] PyProcess - W-81-model-stderr:   warnings.warn(\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-81-model-stdout: LLM sharding and compiling started...\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-81-model-stdout: Loading precompiled graph from /opt/ml/model/./optimized_model/compiled ...\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-81-model-stdout: 2024-Jul-21 05:36:17.0655 81:297 [1] init.cc:96 CCOM WARN Linux kernel 5.10 requires setting FI_EFA_FORK_SAFE=1 environment variable.  Multi-node support will be disabled.\n",
      "INFO:sagemaker:[INFO ] PyProcess - W-81-model-stdout: Please restart with FI_EFA_FORK_SAFE=1 set.\n",
      "INFO:sagemaker:Created endpoint with name meta-textgeneration-llama-3-70b-2024-07-21-05-03-18-831\n",
      "ModelBuilder: DEBUG:     ModelBuilder metrics emitted.\n"
     ]
    }
   ],
   "source": [
    "predictor = optimized_model.deploy(accept_eula=True, model_data_download_timeout=3600, volume_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f221bcb2-b16c-4c2b-8e63-9f684ca1e9e0",
   "metadata": {},
   "source": [
    "Once the deployment has finished successfully, you can send queries to the model by simply using the predictor's `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85c939d2-2b37-4377-92bf-d54e2b3bb467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': \" and I'm here to help you with your questions about the world of technology. Today, I want to talk about the importance of having a strong online presence for your business. In today's digital age, having a website is no longer a luxury, but a necessity. It's the first place potential customers will go to learn more about your business, and it's essential to make a good first impression.\\nBut having a website is just the first step. You also need to make sure that your website is optimized for search engines, so that it can be easily found by potential customers. This is where search engine optimization (SEO) comes in\"}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a23da2a4-b113-44d4-9a59-cbfdeff7a039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: meta-textgeneration-llama-3-70b-2024-07-21-04-23-19-484\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: meta-textgeneration-llama-3-70b-2024-07-21-05-03-18-831\n",
      "INFO:sagemaker:Deleting endpoint with name: meta-textgeneration-llama-3-70b-2024-07-21-05-03-18-831\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
